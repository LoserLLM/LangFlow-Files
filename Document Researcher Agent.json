{
  "data": {
    "edges": [
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-4uvfb",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "LMStudioModel-3OpXe",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatInput-4uvfb{œdataTypeœ:œChatInputœ,œidœ:œChatInput-4uvfbœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-LMStudioModel-3OpXe{œfieldNameœ:œinput_valueœ,œidœ:œLMStudioModel-3OpXeœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatInput-4uvfb",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-4uvfbœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "LMStudioModel-3OpXe",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œLMStudioModel-3OpXeœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LMStudioModel",
            "id": "LMStudioModel-3OpXe",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          },
          "targetHandle": {
            "fieldName": "agent_llm",
            "id": "Agent-zrOq9",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__LMStudioModel-3OpXe{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-3OpXeœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-Agent-zrOq9{œfieldNameœ:œagent_llmœ,œidœ:œAgent-zrOq9œ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "LMStudioModel-3OpXe",
        "sourceHandle": "{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-3OpXeœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "Agent-zrOq9",
        "targetHandle": "{œfieldNameœ:œagent_llmœ,œidœ:œAgent-zrOq9œ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Agent",
            "id": "Agent-zrOq9",
            "name": "response",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-ED2VZ",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__Agent-zrOq9{œdataTypeœ:œAgentœ,œidœ:œAgent-zrOq9œ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-ED2VZ{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-ED2VZœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "Agent-zrOq9",
        "sourceHandle": "{œdataTypeœ:œAgentœ,œidœ:œAgent-zrOq9œ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatOutput-ED2VZ",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-ED2VZœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "FileMetadataTool",
            "id": "CustomComponent-frHjb",
            "name": "component_as_tool",
            "output_types": [
              "Tool"
            ]
          },
          "targetHandle": {
            "fieldName": "tools",
            "id": "Agent-zrOq9",
            "inputTypes": [
              "Tool"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__CustomComponent-frHjb{œdataTypeœ:œFileMetadataToolœ,œidœ:œCustomComponent-frHjbœ,œnameœ:œcomponent_as_toolœ,œoutput_typesœ:[œToolœ]}-Agent-zrOq9{œfieldNameœ:œtoolsœ,œidœ:œAgent-zrOq9œ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "CustomComponent-frHjb",
        "sourceHandle": "{œdataTypeœ:œFileMetadataToolœ,œidœ:œCustomComponent-frHjbœ,œnameœ:œcomponent_as_toolœ,œoutput_typesœ:[œToolœ]}",
        "target": "Agent-zrOq9",
        "targetHandle": "{œfieldNameœ:œtoolsœ,œidœ:œAgent-zrOq9œ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "GrepSearchTool",
            "id": "CustomComponent-km8ac",
            "name": "component_as_tool",
            "output_types": [
              "Tool"
            ]
          },
          "targetHandle": {
            "fieldName": "tools",
            "id": "Agent-zrOq9",
            "inputTypes": [
              "Tool"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__CustomComponent-km8ac{œdataTypeœ:œGrepSearchToolœ,œidœ:œCustomComponent-km8acœ,œnameœ:œcomponent_as_toolœ,œoutput_typesœ:[œToolœ]}-Agent-zrOq9{œfieldNameœ:œtoolsœ,œidœ:œAgent-zrOq9œ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "CustomComponent-km8ac",
        "sourceHandle": "{œdataTypeœ:œGrepSearchToolœ,œidœ:œCustomComponent-km8acœ,œnameœ:œcomponent_as_toolœ,œoutput_typesœ:[œToolœ]}",
        "target": "Agent-zrOq9",
        "targetHandle": "{œfieldNameœ:œtoolsœ,œidœ:œAgent-zrOq9œ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChunkedFileReader",
            "id": "CustomComponent-dSqdc",
            "name": "component_as_tool",
            "output_types": [
              "Tool"
            ]
          },
          "targetHandle": {
            "fieldName": "tools",
            "id": "Agent-zrOq9",
            "inputTypes": [
              "Tool"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__CustomComponent-dSqdc{œdataTypeœ:œChunkedFileReaderœ,œidœ:œCustomComponent-dSqdcœ,œnameœ:œcomponent_as_toolœ,œoutput_typesœ:[œToolœ]}-Agent-zrOq9{œfieldNameœ:œtoolsœ,œidœ:œAgent-zrOq9œ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "CustomComponent-dSqdc",
        "sourceHandle": "{œdataTypeœ:œChunkedFileReaderœ,œidœ:œCustomComponent-dSqdcœ,œnameœ:œcomponent_as_toolœ,œoutput_typesœ:[œToolœ]}",
        "target": "Agent-zrOq9",
        "targetHandle": "{œfieldNameœ:œtoolsœ,œidœ:œAgent-zrOq9œ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "CodeRunnerTool",
            "id": "CustomComponent-RBhOR",
            "name": "component_as_tool",
            "output_types": [
              "Tool"
            ]
          },
          "targetHandle": {
            "fieldName": "tools",
            "id": "Agent-zrOq9",
            "inputTypes": [
              "Tool"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__CustomComponent-RBhOR{œdataTypeœ:œCodeRunnerToolœ,œidœ:œCustomComponent-RBhORœ,œnameœ:œcomponent_as_toolœ,œoutput_typesœ:[œToolœ]}-Agent-zrOq9{œfieldNameœ:œtoolsœ,œidœ:œAgent-zrOq9œ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "CustomComponent-RBhOR",
        "sourceHandle": "{œdataTypeœ:œCodeRunnerToolœ,œidœ:œCustomComponent-RBhORœ,œnameœ:œcomponent_as_toolœ,œoutput_typesœ:[œToolœ]}",
        "target": "Agent-zrOq9",
        "targetHandle": "{œfieldNameœ:œtoolsœ,œidœ:œAgent-zrOq9œ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}"
      }
    ],
    "nodes": [
      {
        "data": {
          "id": "ChatInput-4uvfb",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "category": "inputs",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get chat inputs from the Playground.",
            "display_name": "Chat Input",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "files",
              "background_color",
              "chat_icon",
              "text_color"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "key": "ChatInput",
            "legacy": false,
            "lf_version": "1.6.0",
            "metadata": {},
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Chat Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 0.0020353564437605998,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.io import (\n    DropdownInput,\n    FileInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n)\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_USER,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-input\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n    minimized = True\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Input Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n            input_types=[],\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n            temp_file=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Chat Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    async def message_response(self) -> Message:\n        # Ensure files is a list and filter out empty/None values\n        files = self.files if self.files else []\n        if files and not isinstance(files, list):\n            files = [files]\n        files = [f for f in files if f is not None and f != \"\"]\n\n        message = await Message.create(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=files,\n        )\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = await self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n"
              },
              "files": {
                "_input_type": "FileInput",
                "advanced": true,
                "display_name": "Files",
                "dynamic": false,
                "fileTypes": [
                  "csv",
                  "json",
                  "pdf",
                  "txt",
                  "md",
                  "mdx",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "file_path": "",
                "info": "Files to be sent with the message.",
                "list": true,
                "list_add_label": "Add More",
                "name": "files",
                "placeholder": "",
                "required": false,
                "show": true,
                "temp_file": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "file",
                "value": ""
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Input Text",
                "dynamic": false,
                "info": "Message to be passed as input.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "selected_output": "message",
          "showNode": false,
          "type": "ChatInput"
        },
        "dragging": false,
        "id": "ChatInput-4uvfb",
        "measured": {
          "height": 48,
          "width": 192
        },
        "position": {
          "x": 2129.919531634812,
          "y": -935.6686494628715
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ChatOutput-ED2VZ",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "category": "outputs",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template",
              "background_color",
              "chat_icon",
              "text_color",
              "clean_data"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "key": "ChatOutput",
            "legacy": false,
            "lf_version": "1.6.0",
            "metadata": {},
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 0.003169567463043492,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.helpers.data import safe_convert\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.template.field.base import Output\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, _icon, display_name, source_id = self.get_properties_from_source_component()\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message):\n            message = self.input_value\n            # Update message properties\n            message.text = text\n        else:\n            message = Message(text=text)\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n\n        # Store message if needed\n        if self.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            clean_data: bool = getattr(self, \"clean_data\", False)\n            return \"\\n\".join([safe_convert(item, clean_data=clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_value",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": false,
          "type": "ChatOutput"
        },
        "dragging": false,
        "id": "ChatOutput-ED2VZ",
        "measured": {
          "height": 48,
          "width": 192
        },
        "position": {
          "x": 3356.2559366599785,
          "y": -118.7159878263278
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "LMStudioModel-3OpXe",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "category": "lmstudio",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generate text using LM Studio Local LLMs.",
            "display_name": "LM Studio",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_tokens",
              "model_kwargs",
              "model_name",
              "base_url",
              "api_key",
              "temperature",
              "seed"
            ],
            "frozen": false,
            "icon": "LMStudio",
            "key": "LMStudioModel",
            "last_updated": "2025-11-03T00:36:34.059Z",
            "legacy": false,
            "lf_version": "1.6.0",
            "metadata": {
              "keywords": [
                "model",
                "llm",
                "language model",
                "large language model"
              ]
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "method": "text_response",
                "name": "text_output",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Language Model",
                "group_outputs": false,
                "method": "build_model",
                "name": "model_output",
                "options": null,
                "required_inputs": null,
                "selected": "LanguageModel",
                "tool_mode": true,
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 5.181793099517413e-13,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": true,
                "display_name": "LM Studio API Key",
                "dynamic": false,
                "info": "The LM Studio API Key to use for LM Studio.",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "base_url": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Base URL",
                "dynamic": false,
                "info": "Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "http://192.168.50.117:1234/v1"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_openai import ChatOpenAI\nfrom typing_extensions import override\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs.inputs import DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\n\n\nclass LMStudioModelComponent(LCModelComponent):\n    display_name = \"LM Studio\"\n    description = \"Generate text using LM Studio Local LLMs.\"\n    icon = \"LMStudio\"\n    name = \"LMStudioModel\"\n\n    @override\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = await self.get_variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:1234/v1\"\n            build_config[\"model_name\"][\"options\"] = await self.get_model(base_url_value)\n\n        return build_config\n\n    @staticmethod\n    async def get_model(base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/v1/models\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"id\"] for model in data.get(\"data\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure the LM Studio server is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            advanced=False,\n            info=\"Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.\",\n            value=\"http://localhost:1234/v1\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"LM Studio API Key\",\n            info=\"The LM Studio API Key to use for LM Studio.\",\n            advanced=True,\n            value=\"LMSTUDIO_API_KEY\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        lmstudio_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        base_url = self.base_url or \"http://localhost:1234/v1\"\n        seed = self.seed\n\n        return ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=base_url,\n            api_key=lmstudio_api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an LM Studio exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "max_tokens": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Tokens",
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_tokens",
                "placeholder": "",
                "range_spec": {
                  "max": 128000,
                  "min": 0,
                  "step": 0.1,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": ""
              },
              "model_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Model Kwargs",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "model_kwargs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Name",
                "dynamic": false,
                "external_options": {},
                "info": "",
                "name": "model_name",
                "options": [
                  "mlx-community/gpt-oss-120b",
                  "mlx-community/gpt-oss-20b",
                  "grok-2",
                  "kimi-linear-48b-a3b-instruct@6bit",
                  "minimax-m2@8bit",
                  "kimi-linear-48b-a3b-instruct@8bit",
                  "minimax-m2@4bit",
                  "qwen/qwen3-vl-4b",
                  "kimi-k2-instruct-0905-mlx",
                  "mistralai/magistral-small",
                  "mistralai/magistral-small-2509",
                  "ibm/granite-4-h-tiny",
                  "baidu/ernie-4.5-21b-a3b",
                  "qwen/qwen2.5-vl-7b",
                  "google/gemma-3-4b",
                  "qwen/qwen3-4b-2507",
                  "meta/llama-3.3-70b",
                  "glm-4.5-air-mlx",
                  "qwen/qwen3-next-80b",
                  "qwen/qwen3-vl-30b",
                  "qwen/qwen3-30b-a3b-2507",
                  "glm-4.5-air",
                  "thudm_glm-4-32b-0414",
                  "glm-4.6@q4_k_s",
                  "glm-4.6@q8_k_xl",
                  "glm-z1-rumination-32b-0414",
                  "mistralai/mistral-7b-instruct-v0.3",
                  "google/gemma-3-27b",
                  "qwen/qwen3-vl-8b",
                  "bytedance/seed-oss-36b",
                  "ling-1t-mlx-dq3_k_m",
                  "ling-1t",
                  "qwen3-30b-a3b",
                  "text-embedding-nomic-embed-text-v1.5",
                  "qwen3-30b-a3b-instruct-2507",
                  "deepseek-r1-distill-llama-8b@bf16",
                  "deepseek-r1-distill-llama-8b@q4_k_s",
                  "deepseek-r1-0528-qwen3-8b@bf16",
                  "deepseek-r1-0528-qwen3-8b@q4_k_s",
                  "unsloth/gpt-oss-120b",
                  "ui-tars-72b-dpo@q3_k_l",
                  "ui-tars-72b-dpo@q4_k_m",
                  "ui-tars-72b-dpo@q6_k",
                  "ui-tars-72b-dpo@q8_0",
                  "ui-tars-7b-sft@q4_1",
                  "ui-tars-7b-sft@q4_k_s",
                  "ui-tars-7b-sft@q5_k_m",
                  "ui-tars-7b-sft@q5_k_s",
                  "ui-tars-7b-sft@q6_k",
                  "ui-tars-7b-sft@q6_k_l",
                  "ui-tars-7b-sft@q8_0",
                  "ui-tars-7b-sft@f16",
                  "ui-tars-7b-sft@f32",
                  "qwen3-next-80b-a3b-instruct",
                  "ui-tars-72b-dpo@?",
                  "deepseek-r1@q3_k_m",
                  "deepseek-r1@q4_k_m",
                  "ui-tars-72b-dpo-i1",
                  "ui-tars-72b-dpo@6bit",
                  "ui-tars-72b-dpo@8bit",
                  "openai_gpt-oss-20b-coder-neo-code-di-matrix",
                  "ui-tars-72b-dpo@4bit",
                  "unsloth/seed-oss-36b-instruct",
                  "longcat-flash-chat",
                  "mlx-community/ui-tars-1.5-7b",
                  "gemmasutra-pro-27b-v1.1",
                  "mungert/ui-tars-1.5-7b",
                  "lmstudio-community/gemma-3-12b-it",
                  "unsloth/gemma-3-12b-it",
                  "openhands-lm-32b-v0.1",
                  "all-hands_openhands-lm-32b-v0.1",
                  "lmstudio-community/r1-1776",
                  "glm-4.5",
                  "exaone-4.0.1-32b",
                  "qwen3-235b-a22b-instruct-2507",
                  "internvl3_5-30b-a3b",
                  "hermes-4-405b-mlx",
                  "cogito-v2-preview-llama-109b-moe",
                  "mindlink-32b-0801",
                  "lmstudio-community/llama-4-scout-17b-16e-instruct",
                  "meta-llama-3-70b-instruct",
                  "mlx-community/qwen3-coder-480b-a35b-instruct",
                  "xwin-lm-70b-v0.1",
                  "deepseek-r1-distill-qwen-32b-abliterated",
                  "lmstudio-community/meta-llama-3.1-8b-instruct",
                  "nousresearch/hermes-4-70b",
                  "qwen3-4b-instruct-2507",
                  "wizardcoder-python-34b-v1.0",
                  "kimi-k2-instruct-q2ks-mixed-autoround",
                  "unsloth/r1-1776",
                  "kimi-k2-instruct-dq4_k",
                  "mlx-community/kimi-k2-instruct",
                  "mlx-community/deepseek-v3.1",
                  "kimi-dev-72b",
                  "starcoder",
                  "deepseek-v3.1-mlx-5",
                  "kimi-k2-instruct-mlx-3",
                  "deepseek-v3-0324",
                  "deepseek-r1-distill-qwen-32b-mlx",
                  "qwen/qwen3-235b-a22b-2507",
                  "codellama-70b-instruct-hf-mlx",
                  "mistralai/mistral-small-3.2",
                  "google/gemma-3n-e4b",
                  "qwen/qwq-32b",
                  "mistralai/devstral-small-2505",
                  "qwen/qwen2.5-coder-14b",
                  "deepseek/deepseek-r1-0528-qwen3-8b",
                  "qwen/qwen3-coder-30b",
                  "microsoft/phi-4-reasoning-plus",
                  "liquid/lfm2-1.2b",
                  "qwen/qwen3-coder-480b",
                  "deepseek-v3.1@q3_k_m",
                  "deepseek-v3.1@q3_k_s",
                  "qwen3-coder-30b-a3b-instruct-480b-distill-v2",
                  "qwen2.5-vl-7b-instruct@bf16",
                  "qwen2.5-vl-7b-instruct@q4_k_s",
                  "cydonia-24b-v4.1",
                  "huihui-gpt-oss-20b-abliterated",
                  "mlx-community/deepseek-r1",
                  "lfm2-vl-1.6b",
                  "lfm2-vl-450m",
                  "jan-v1-4b",
                  "deepseek-r1-distill-qwen-32b",
                  "llama-3-8b-lexi-uncensored",
                  "qwq-32b",
                  "bartowski/meta-llama-3.1-8b-instruct",
                  "hermes-2-pro-mistral-7b",
                  "omnivlm-968m",
                  "unsloth/qwen3-coder-480b-a35b-instruct",
                  "unsloth/gpt-oss-20b",
                  "unsloth/kimi-k2-instruct",
                  "openai_gpt-oss-120b-neo-imatrix",
                  "llama-4-maverick-17b-128e-instruct",
                  "security-attacks-mitre",
                  "openai-gpt-oss-20b-abliterated-uncensored-neo-imatrix",
                  "unsloth/llama-4-scout-17b-16e-instruct",
                  "deepseek-v3",
                  "llama-3.2-3b-instruct",
                  "openai/gpt-oss-120b",
                  "qwen3-coder-30b-a3b-instruct",
                  "gemma-3-270m-it",
                  "meta-llama-3-8b-instruct",
                  "openai/gpt-oss-20b",
                  "qwen/qwen3-4b-thinking-2507"
                ],
                "options_metadata": [],
                "placeholder": "",
                "refresh_button": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "qwen3-next-80b-a3b-instruct"
              },
              "seed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Seed",
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "list": false,
                "list_add_label": "Add More",
                "name": "seed",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "System message to pass to the model.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "temperature",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.1
              }
            },
            "tool_mode": false
          },
          "selected_output": "model_output",
          "showNode": true,
          "type": "LMStudioModel"
        },
        "dragging": false,
        "id": "LMStudioModel-3OpXe",
        "measured": {
          "height": 449,
          "width": 320
        },
        "position": {
          "x": 2453.7097058747236,
          "y": -1022.9614856610451
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Agent-zrOq9",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Define the agent's instructions, then enter a task to complete using tools.",
            "display_name": "Agent",
            "documentation": "https://docs.langflow.org/agents",
            "edited": false,
            "field_order": [
              "agent_llm",
              "max_tokens",
              "model_kwargs",
              "model_name",
              "openai_api_base",
              "api_key",
              "temperature",
              "seed",
              "max_retries",
              "timeout",
              "system_prompt",
              "n_messages",
              "format_instructions",
              "output_schema",
              "tools",
              "input_value",
              "handle_parsing_errors",
              "verbose",
              "max_iterations",
              "agent_description",
              "add_current_date_tool"
            ],
            "frozen": false,
            "icon": "bot",
            "last_updated": "2025-11-10T20:26:10.344Z",
            "legacy": false,
            "lf_version": "1.6.0",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Response",
                "group_outputs": false,
                "method": "message_response",
                "name": "response",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "add_current_date_tool": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Current Date",
                "dynamic": false,
                "info": "If true, will add a tool to the agent that returns the current date.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "add_current_date_tool",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "agent_description": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "copy_field": false,
                "display_name": "Agent Description [Deprecated]",
                "dynamic": false,
                "info": "The description of the agent. This is only used when in Tool Mode. Defaults to 'A helpful assistant with access to the following tools:' and tools are added dynamically. This feature is deprecated and will be removed in future versions.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "agent_description",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "A helpful assistant with access to the following tools:"
              },
              "agent_llm": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Language Model",
                "dynamic": false,
                "external_options": {
                  "fields": {
                    "data": {
                      "node": {
                        "display_name": "Connect other models",
                        "icon": "CornerDownLeft",
                        "name": "connect_other_models"
                      }
                    }
                  }
                },
                "info": "The provider of the language model that the agent will use to generate responses.",
                "input_types": [
                  "LanguageModel"
                ],
                "name": "agent_llm",
                "options": [
                  "Anthropic",
                  "Google Generative AI",
                  "OpenAI"
                ],
                "options_metadata": [
                  {
                    "icon": "Anthropic"
                  },
                  {
                    "icon": "GoogleGenerativeAI"
                  },
                  {
                    "icon": "OpenAI"
                  }
                ],
                "placeholder": "Awaiting model input.",
                "real_time_refresh": true,
                "refresh_button": false,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import json\nimport re\n\nfrom langchain_core.tools import StructuredTool\nfrom pydantic import ValidationError\n\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.base.agents.events import ExceptionWithMessageError\nfrom langflow.base.models.model_input_constants import (\n    ALL_PROVIDER_FIELDS,\n    MODEL_DYNAMIC_UPDATE_FIELDS,\n    MODEL_PROVIDERS_DICT,\n    MODELS_METADATA,\n)\nfrom langflow.base.models.model_utils import get_model_name\nfrom langflow.components.helpers.current_date import CurrentDateComponent\nfrom langflow.components.helpers.memory import MemoryComponent\nfrom langflow.components.langchain_utilities.tool_calling import (\n    ToolCallingAgentComponent,\n)\nfrom langflow.custom.custom_component.component import _get_component_toolkit\nfrom langflow.custom.utils import update_component_build_config\nfrom langflow.field_typing import Tool\nfrom langflow.helpers.base_model import build_model_from_schema\nfrom langflow.io import (\n    BoolInput,\n    DropdownInput,\n    IntInput,\n    MultilineInput,\n    Output,\n    TableInput,\n)\nfrom langflow.logging import logger\nfrom langflow.schema.data import Data\nfrom langflow.schema.dotdict import dotdict\nfrom langflow.schema.message import Message\nfrom langflow.schema.table import EditMode\n\n\ndef set_advanced_true(component_input):\n    component_input.advanced = True\n    return component_input\n\n\nMODEL_PROVIDERS_LIST = [\"Anthropic\", \"Google Generative AI\", \"OpenAI\"]\n\n\nclass AgentComponent(ToolCallingAgentComponent):\n    display_name: str = \"Agent\"\n    description: str = \"Define the agent's instructions, then enter a task to complete using tools.\"\n    documentation: str = \"https://docs.langflow.org/agents\"\n    icon = \"bot\"\n    beta = False\n    name = \"Agent\"\n\n    memory_inputs = [set_advanced_true(component_input) for component_input in MemoryComponent().inputs]\n\n    # Filter out json_mode from OpenAI inputs since we handle structured output differently\n    openai_inputs_filtered = [\n        input_field\n        for input_field in MODEL_PROVIDERS_DICT[\"OpenAI\"][\"inputs\"]\n        if not (hasattr(input_field, \"name\") and input_field.name == \"json_mode\")\n    ]\n\n    inputs = [\n        DropdownInput(\n            name=\"agent_llm\",\n            display_name=\"Model Provider\",\n            info=\"The provider of the language model that the agent will use to generate responses.\",\n            options=[*MODEL_PROVIDERS_LIST],\n            value=\"OpenAI\",\n            real_time_refresh=True,\n            refresh_button=False,\n            input_types=[],\n            options_metadata=[MODELS_METADATA[key] for key in MODEL_PROVIDERS_LIST],\n            external_options={\n                \"fields\": {\n                    \"data\": {\n                        \"node\": {\n                            \"name\": \"connect_other_models\",\n                            \"display_name\": \"Connect other models\",\n                            \"icon\": \"CornerDownLeft\",\n                        }\n                    }\n                },\n            },\n        ),\n        *openai_inputs_filtered,\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"Agent Instructions\",\n            info=\"System Prompt: Initial instructions and context provided to guide the agent's behavior.\",\n            value=\"You are a helpful assistant that can use tools to answer questions and perform tasks.\",\n            advanced=False,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Chat History Messages\",\n            value=100,\n            info=\"Number of chat history messages to retrieve.\",\n            advanced=True,\n            show=True,\n        ),\n        MultilineInput(\n            name=\"format_instructions\",\n            display_name=\"Output Format Instructions\",\n            info=\"Generic Template for structured output formatting. Valid only with Structured response.\",\n            value=(\n                \"You are an AI that extracts structured JSON objects from unstructured text. \"\n                \"Use a predefined schema with expected types (str, int, float, bool, dict). \"\n                \"Extract ALL relevant instances that match the schema - if multiple patterns exist, capture them all. \"\n                \"Fill missing or ambiguous values with defaults: null for missing values. \"\n                \"Remove exact duplicates but keep variations that have different field values. \"\n                \"Always return valid JSON in the expected format, never throw errors. \"\n                \"If multiple objects can be extracted, return them all in the structured format.\"\n            ),\n            advanced=True,\n        ),\n        TableInput(\n            name=\"output_schema\",\n            display_name=\"Output Schema\",\n            info=(\n                \"Schema Validation: Define the structure and data types for structured output. \"\n                \"No validation if no output schema.\"\n            ),\n            advanced=True,\n            required=False,\n            value=[],\n            table_schema=[\n                {\n                    \"name\": \"name\",\n                    \"display_name\": \"Name\",\n                    \"type\": \"str\",\n                    \"description\": \"Specify the name of the output field.\",\n                    \"default\": \"field\",\n                    \"edit_mode\": EditMode.INLINE,\n                },\n                {\n                    \"name\": \"description\",\n                    \"display_name\": \"Description\",\n                    \"type\": \"str\",\n                    \"description\": \"Describe the purpose of the output field.\",\n                    \"default\": \"description of field\",\n                    \"edit_mode\": EditMode.POPOVER,\n                },\n                {\n                    \"name\": \"type\",\n                    \"display_name\": \"Type\",\n                    \"type\": \"str\",\n                    \"edit_mode\": EditMode.INLINE,\n                    \"description\": (\"Indicate the data type of the output field (e.g., str, int, float, bool, dict).\"),\n                    \"options\": [\"str\", \"int\", \"float\", \"bool\", \"dict\"],\n                    \"default\": \"str\",\n                },\n                {\n                    \"name\": \"multiple\",\n                    \"display_name\": \"As List\",\n                    \"type\": \"boolean\",\n                    \"description\": \"Set to True if this output field should be a list of the specified type.\",\n                    \"default\": \"False\",\n                    \"edit_mode\": EditMode.INLINE,\n                },\n            ],\n        ),\n        *LCToolsAgentComponent._base_inputs,\n        # removed memory inputs from agent component\n        # *memory_inputs,\n        BoolInput(\n            name=\"add_current_date_tool\",\n            display_name=\"Current Date\",\n            advanced=True,\n            info=\"If true, will add a tool to the agent that returns the current date.\",\n            value=True,\n        ),\n    ]\n    outputs = [\n        Output(name=\"response\", display_name=\"Response\", method=\"message_response\"),\n    ]\n\n    async def get_agent_requirements(self):\n        \"\"\"Get the agent requirements for the agent.\"\"\"\n        llm_model, display_name = await self.get_llm()\n        if llm_model is None:\n            msg = \"No language model selected. Please choose a model to proceed.\"\n            raise ValueError(msg)\n        self.model_name = get_model_name(llm_model, display_name=display_name)\n\n        # Get memory data\n        self.chat_history = await self.get_memory_data()\n        if isinstance(self.chat_history, Message):\n            self.chat_history = [self.chat_history]\n\n        # Add current date tool if enabled\n        if self.add_current_date_tool:\n            if not isinstance(self.tools, list):  # type: ignore[has-type]\n                self.tools = []\n            current_date_tool = (await CurrentDateComponent(**self.get_base_args()).to_toolkit()).pop(0)\n            if not isinstance(current_date_tool, StructuredTool):\n                msg = \"CurrentDateComponent must be converted to a StructuredTool\"\n                raise TypeError(msg)\n            self.tools.append(current_date_tool)\n        return llm_model, self.chat_history, self.tools\n\n    async def message_response(self) -> Message:\n        try:\n            llm_model, self.chat_history, self.tools = await self.get_agent_requirements()\n            # Set up and run agent\n            self.set(\n                llm=llm_model,\n                tools=self.tools or [],\n                chat_history=self.chat_history,\n                input_value=self.input_value,\n                system_prompt=self.system_prompt,\n            )\n            agent = self.create_agent_runnable()\n            result = await self.run_agent(agent)\n\n            # Store result for potential JSON output\n            self._agent_result = result\n\n        except (ValueError, TypeError, KeyError) as e:\n            await logger.aerror(f\"{type(e).__name__}: {e!s}\")\n            raise\n        except ExceptionWithMessageError as e:\n            await logger.aerror(f\"ExceptionWithMessageError occurred: {e}\")\n            raise\n        # Avoid catching blind Exception; let truly unexpected exceptions propagate\n        except Exception as e:\n            await logger.aerror(f\"Unexpected error: {e!s}\")\n            raise\n        else:\n            return result\n\n    def _preprocess_schema(self, schema):\n        \"\"\"Preprocess schema to ensure correct data types for build_model_from_schema.\"\"\"\n        processed_schema = []\n        for field in schema:\n            processed_field = {\n                \"name\": str(field.get(\"name\", \"field\")),\n                \"type\": str(field.get(\"type\", \"str\")),\n                \"description\": str(field.get(\"description\", \"\")),\n                \"multiple\": field.get(\"multiple\", False),\n            }\n            # Ensure multiple is handled correctly\n            if isinstance(processed_field[\"multiple\"], str):\n                processed_field[\"multiple\"] = processed_field[\"multiple\"].lower() in [\n                    \"true\",\n                    \"1\",\n                    \"t\",\n                    \"y\",\n                    \"yes\",\n                ]\n            processed_schema.append(processed_field)\n        return processed_schema\n\n    async def build_structured_output_base(self, content: str):\n        \"\"\"Build structured output with optional BaseModel validation.\"\"\"\n        json_pattern = r\"\\{.*\\}\"\n        schema_error_msg = \"Try setting an output schema\"\n\n        # Try to parse content as JSON first\n        json_data = None\n        try:\n            json_data = json.loads(content)\n        except json.JSONDecodeError:\n            json_match = re.search(json_pattern, content, re.DOTALL)\n            if json_match:\n                try:\n                    json_data = json.loads(json_match.group())\n                except json.JSONDecodeError:\n                    return {\"content\": content, \"error\": schema_error_msg}\n            else:\n                return {\"content\": content, \"error\": schema_error_msg}\n\n        # If no output schema provided, return parsed JSON without validation\n        if not hasattr(self, \"output_schema\") or not self.output_schema or len(self.output_schema) == 0:\n            return json_data\n\n        # Use BaseModel validation with schema\n        try:\n            processed_schema = self._preprocess_schema(self.output_schema)\n            output_model = build_model_from_schema(processed_schema)\n\n            # Validate against the schema\n            if isinstance(json_data, list):\n                # Multiple objects\n                validated_objects = []\n                for item in json_data:\n                    try:\n                        validated_obj = output_model.model_validate(item)\n                        validated_objects.append(validated_obj.model_dump())\n                    except ValidationError as e:\n                        await logger.aerror(f\"Validation error for item: {e}\")\n                        # Include invalid items with error info\n                        validated_objects.append({\"data\": item, \"validation_error\": str(e)})\n                return validated_objects\n\n            # Single object\n            try:\n                validated_obj = output_model.model_validate(json_data)\n                return [validated_obj.model_dump()]  # Return as list for consistency\n            except ValidationError as e:\n                await logger.aerror(f\"Validation error: {e}\")\n                return [{\"data\": json_data, \"validation_error\": str(e)}]\n\n        except (TypeError, ValueError) as e:\n            await logger.aerror(f\"Error building structured output: {e}\")\n            # Fallback to parsed JSON without validation\n            return json_data\n\n    async def json_response(self) -> Data:\n        \"\"\"Convert agent response to structured JSON Data output with schema validation.\"\"\"\n        # Always use structured chat agent for JSON response mode for better JSON formatting\n        try:\n            system_components = []\n\n            # 1. Agent Instructions (system_prompt)\n            agent_instructions = getattr(self, \"system_prompt\", \"\") or \"\"\n            if agent_instructions:\n                system_components.append(f\"{agent_instructions}\")\n\n            # 2. Format Instructions\n            format_instructions = getattr(self, \"format_instructions\", \"\") or \"\"\n            if format_instructions:\n                system_components.append(f\"Format instructions: {format_instructions}\")\n\n            # 3. Schema Information from BaseModel\n            if hasattr(self, \"output_schema\") and self.output_schema and len(self.output_schema) > 0:\n                try:\n                    processed_schema = self._preprocess_schema(self.output_schema)\n                    output_model = build_model_from_schema(processed_schema)\n                    schema_dict = output_model.model_json_schema()\n                    schema_info = (\n                        \"You are given some text that may include format instructions, \"\n                        \"explanations, or other content alongside a JSON schema.\\n\\n\"\n                        \"Your task:\\n\"\n                        \"- Extract only the JSON schema.\\n\"\n                        \"- Return it as valid JSON.\\n\"\n                        \"- Do not include format instructions, explanations, or extra text.\\n\\n\"\n                        \"Input:\\n\"\n                        f\"{json.dumps(schema_dict, indent=2)}\\n\\n\"\n                        \"Output (only JSON schema):\"\n                    )\n                    system_components.append(schema_info)\n                except (ValidationError, ValueError, TypeError, KeyError) as e:\n                    await logger.aerror(f\"Could not build schema for prompt: {e}\", exc_info=True)\n\n            # Combine all components\n            combined_instructions = \"\\n\\n\".join(system_components) if system_components else \"\"\n            llm_model, self.chat_history, self.tools = await self.get_agent_requirements()\n            self.set(\n                llm=llm_model,\n                tools=self.tools or [],\n                chat_history=self.chat_history,\n                input_value=self.input_value,\n                system_prompt=combined_instructions,\n            )\n\n            # Create and run structured chat agent\n            try:\n                structured_agent = self.create_agent_runnable()\n            except (NotImplementedError, ValueError, TypeError) as e:\n                await logger.aerror(f\"Error with structured chat agent: {e}\")\n                raise\n            try:\n                result = await self.run_agent(structured_agent)\n            except (\n                ExceptionWithMessageError,\n                ValueError,\n                TypeError,\n                RuntimeError,\n            ) as e:\n                await logger.aerror(f\"Error with structured agent result: {e}\")\n                raise\n            # Extract content from structured agent result\n            if hasattr(result, \"content\"):\n                content = result.content\n            elif hasattr(result, \"text\"):\n                content = result.text\n            else:\n                content = str(result)\n\n        except (\n            ExceptionWithMessageError,\n            ValueError,\n            TypeError,\n            NotImplementedError,\n            AttributeError,\n        ) as e:\n            await logger.aerror(f\"Error with structured chat agent: {e}\")\n            # Fallback to regular agent\n            content_str = \"No content returned from agent\"\n            return Data(data={\"content\": content_str, \"error\": str(e)})\n\n        # Process with structured output validation\n        try:\n            structured_output = await self.build_structured_output_base(content)\n\n            # Handle different output formats\n            if isinstance(structured_output, list) and structured_output:\n                if len(structured_output) == 1:\n                    return Data(data=structured_output[0])\n                return Data(data={\"results\": structured_output})\n            if isinstance(structured_output, dict):\n                return Data(data=structured_output)\n            return Data(data={\"content\": content})\n\n        except (ValueError, TypeError) as e:\n            await logger.aerror(f\"Error in structured output processing: {e}\")\n            return Data(data={\"content\": content, \"error\": str(e)})\n\n    async def get_memory_data(self):\n        # TODO: This is a temporary fix to avoid message duplication. We should develop a function for this.\n        messages = (\n            await MemoryComponent(**self.get_base_args())\n            .set(\n                session_id=self.graph.session_id,\n                order=\"Ascending\",\n                n_messages=self.n_messages,\n            )\n            .retrieve_messages()\n        )\n        return [\n            message for message in messages if getattr(message, \"id\", None) != getattr(self.input_value, \"id\", None)\n        ]\n\n    async def get_llm(self):\n        if not isinstance(self.agent_llm, str):\n            return self.agent_llm, None\n\n        try:\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if not provider_info:\n                msg = f\"Invalid model provider: {self.agent_llm}\"\n                raise ValueError(msg)\n\n            component_class = provider_info.get(\"component_class\")\n            display_name = component_class.display_name\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\", \"\")\n\n            return self._build_llm_model(component_class, inputs, prefix), display_name\n\n        except (AttributeError, ValueError, TypeError, RuntimeError) as e:\n            await logger.aerror(f\"Error building {self.agent_llm} language model: {e!s}\")\n            msg = f\"Failed to initialize language model: {e!s}\"\n            raise ValueError(msg) from e\n\n    def _build_llm_model(self, component, inputs, prefix=\"\"):\n        model_kwargs = {}\n        for input_ in inputs:\n            if hasattr(self, f\"{prefix}{input_.name}\"):\n                model_kwargs[input_.name] = getattr(self, f\"{prefix}{input_.name}\")\n        return component.set(**model_kwargs).build_model()\n\n    def set_component_params(self, component):\n        provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n        if provider_info:\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\")\n            # Filter out json_mode and only use attributes that exist on this component\n            model_kwargs = {}\n            for input_ in inputs:\n                if hasattr(self, f\"{prefix}{input_.name}\"):\n                    model_kwargs[input_.name] = getattr(self, f\"{prefix}{input_.name}\")\n\n            return component.set(**model_kwargs)\n        return component\n\n    def delete_fields(self, build_config: dotdict, fields: dict | list[str]) -> None:\n        \"\"\"Delete specified fields from build_config.\"\"\"\n        for field in fields:\n            build_config.pop(field, None)\n\n    def update_input_types(self, build_config: dotdict) -> dotdict:\n        \"\"\"Update input types for all fields in build_config.\"\"\"\n        for key, value in build_config.items():\n            if isinstance(value, dict):\n                if value.get(\"input_types\") is None:\n                    build_config[key][\"input_types\"] = []\n            elif hasattr(value, \"input_types\") and value.input_types is None:\n                value.input_types = []\n        return build_config\n\n    async def update_build_config(\n        self, build_config: dotdict, field_value: str, field_name: str | None = None\n    ) -> dotdict:\n        # Iterate over all providers in the MODEL_PROVIDERS_DICT\n        # Existing logic for updating build_config\n        if field_name in (\"agent_llm\",):\n            build_config[\"agent_llm\"][\"value\"] = field_value\n            provider_info = MODEL_PROVIDERS_DICT.get(field_value)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call the component class's update_build_config method\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, \"model_name\"\n                    )\n\n            provider_configs: dict[str, tuple[dict, list[dict]]] = {\n                provider: (\n                    MODEL_PROVIDERS_DICT[provider][\"fields\"],\n                    [\n                        MODEL_PROVIDERS_DICT[other_provider][\"fields\"]\n                        for other_provider in MODEL_PROVIDERS_DICT\n                        if other_provider != provider\n                    ],\n                )\n                for provider in MODEL_PROVIDERS_DICT\n            }\n            if field_value in provider_configs:\n                fields_to_add, fields_to_delete = provider_configs[field_value]\n\n                # Delete fields from other providers\n                for fields in fields_to_delete:\n                    self.delete_fields(build_config, fields)\n\n                # Add provider-specific fields\n                if field_value == \"OpenAI\" and not any(field in build_config for field in fields_to_add):\n                    build_config.update(fields_to_add)\n                else:\n                    build_config.update(fields_to_add)\n                # Reset input types for agent_llm\n                build_config[\"agent_llm\"][\"input_types\"] = []\n                build_config[\"agent_llm\"][\"display_name\"] = \"Model Provider\"\n            elif field_value == \"connect_other_models\":\n                # Delete all provider fields\n                self.delete_fields(build_config, ALL_PROVIDER_FIELDS)\n                # # Update with custom component\n                custom_component = DropdownInput(\n                    name=\"agent_llm\",\n                    display_name=\"Language Model\",\n                    info=\"The provider of the language model that the agent will use to generate responses.\",\n                    options=[*MODEL_PROVIDERS_LIST],\n                    real_time_refresh=True,\n                    refresh_button=False,\n                    input_types=[\"LanguageModel\"],\n                    placeholder=\"Awaiting model input.\",\n                    options_metadata=[MODELS_METADATA[key] for key in MODEL_PROVIDERS_LIST],\n                    external_options={\n                        \"fields\": {\n                            \"data\": {\n                                \"node\": {\n                                    \"name\": \"connect_other_models\",\n                                    \"display_name\": \"Connect other models\",\n                                    \"icon\": \"CornerDownLeft\",\n                                },\n                            }\n                        },\n                    },\n                )\n                build_config.update({\"agent_llm\": custom_component.to_dict()})\n            # Update input types for all fields\n            build_config = self.update_input_types(build_config)\n\n            # Validate required keys\n            default_keys = [\n                \"code\",\n                \"_type\",\n                \"agent_llm\",\n                \"tools\",\n                \"input_value\",\n                \"add_current_date_tool\",\n                \"system_prompt\",\n                \"agent_description\",\n                \"max_iterations\",\n                \"handle_parsing_errors\",\n                \"verbose\",\n            ]\n            missing_keys = [key for key in default_keys if key not in build_config]\n            if missing_keys:\n                msg = f\"Missing required keys in build_config: {missing_keys}\"\n                raise ValueError(msg)\n        if (\n            isinstance(self.agent_llm, str)\n            and self.agent_llm in MODEL_PROVIDERS_DICT\n            and field_name in MODEL_DYNAMIC_UPDATE_FIELDS\n        ):\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                component_class = self.set_component_params(component_class)\n                prefix = provider_info.get(\"prefix\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call each component class's update_build_config method\n                    # remove the prefix from the field_name\n                    if isinstance(field_name, str) and isinstance(prefix, str):\n                        field_name = field_name.replace(prefix, \"\")\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, \"model_name\"\n                    )\n        return dotdict({k: v.to_dict() if hasattr(v, \"to_dict\") else v for k, v in build_config.items()})\n\n    async def _get_tools(self) -> list[Tool]:\n        component_toolkit = _get_component_toolkit()\n        tools_names = self._build_tools_names()\n        agent_description = self.get_tool_description()\n        # TODO: Agent Description Depreciated Feature to be removed\n        description = f\"{agent_description}{tools_names}\"\n        tools = component_toolkit(component=self).get_tools(\n            tool_name=\"Call_Agent\",\n            tool_description=description,\n            callbacks=self.get_langchain_callbacks(),\n        )\n        if hasattr(self, \"tools_metadata\"):\n            tools = component_toolkit(component=self, metadata=self.tools_metadata).update_tools_metadata(tools=tools)\n        return tools\n"
              },
              "format_instructions": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "copy_field": false,
                "display_name": "Output Format Instructions",
                "dynamic": false,
                "info": "Generic Template for structured output formatting. Valid only with Structured response.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "format_instructions",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "You are an AI that extracts structured JSON objects from unstructured text. Use a predefined schema with expected types (str, int, float, bool, dict). Extract ALL relevant instances that match the schema - if multiple patterns exist, capture them all. Fill missing or ambiguous values with defaults: null for missing values. Remove exact duplicates but keep variations that have different field values. Always return valid JSON in the expected format, never throw errors. If multiple objects can be extracted, return them all in the structured format."
              },
              "handle_parsing_errors": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Handle Parse Errors",
                "dynamic": false,
                "info": "Should the Agent fix errors when reading user input for better processing?",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "handle_parsing_errors",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "The input provided by the user for the agent to process.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "max_iterations": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Iterations",
                "dynamic": false,
                "info": "The maximum number of attempts the agent can make to complete its task before it stops.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "max_iterations",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 158
              },
              "n_messages": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Chat History Messages",
                "dynamic": false,
                "info": "Number of chat history messages to retrieve.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "n_messages",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 100
              },
              "output_schema": {
                "_input_type": "TableInput",
                "advanced": true,
                "display_name": "Output Schema",
                "dynamic": false,
                "info": "Schema Validation: Define the structure and data types for structured output. No validation if no output schema.",
                "input_types": [],
                "is_list": true,
                "list_add_label": "Add More",
                "name": "output_schema",
                "placeholder": "",
                "required": false,
                "show": true,
                "table_icon": "Table",
                "table_schema": {
                  "columns": [
                    {
                      "default": "field",
                      "description": "Specify the name of the output field.",
                      "disable_edit": false,
                      "display_name": "Name",
                      "edit_mode": "inline",
                      "filterable": true,
                      "formatter": "text",
                      "hidden": false,
                      "name": "name",
                      "sortable": true,
                      "type": "str"
                    },
                    {
                      "default": "description of field",
                      "description": "Describe the purpose of the output field.",
                      "disable_edit": false,
                      "display_name": "Description",
                      "edit_mode": "popover",
                      "filterable": true,
                      "formatter": "text",
                      "hidden": false,
                      "name": "description",
                      "sortable": true,
                      "type": "str"
                    },
                    {
                      "default": "str",
                      "description": "Indicate the data type of the output field (e.g., str, int, float, bool, dict).",
                      "disable_edit": false,
                      "display_name": "Type",
                      "edit_mode": "inline",
                      "filterable": true,
                      "formatter": "text",
                      "hidden": false,
                      "name": "type",
                      "options": [
                        "str",
                        "int",
                        "float",
                        "bool",
                        "dict"
                      ],
                      "sortable": true,
                      "type": "str"
                    },
                    {
                      "default": false,
                      "description": "Set to True if this output field should be a list of the specified type.",
                      "disable_edit": false,
                      "display_name": "As List",
                      "edit_mode": "inline",
                      "filterable": true,
                      "formatter": "boolean",
                      "hidden": false,
                      "name": "multiple",
                      "sortable": true,
                      "type": "boolean"
                    }
                  ]
                },
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "trigger_icon": "Table",
                "trigger_text": "Open table",
                "type": "table",
                "value": []
              },
              "system_prompt": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Agent Instructions",
                "dynamic": false,
                "info": "System Prompt: Initial instructions and context provided to guide the agent's behavior.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_prompt",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "# Document Researcher Agent - System Prompt\n\nYou are a **Document Researcher Agent** specialized in analyzing and extracting information from documents of any size, including files that exceed the model's context window.\n\n## Your Capabilities\n\nYou have access to the following tools:\n\n### 1. **FileMetadataTool**\n- **Purpose**: Get file size, line count, and processing recommendations BEFORE reading\n- **Use this FIRST** for every file to understand its size and get strategic recommendations\n- **Parameters**:\n  - `file_path`: Path to the file\n  - `include_recommendations`: true (always)\n  - `count_lines`: true (always)\n- **Returns**: File metadata including line count, size, and recommended processing strategy\n\n### 2. **GrepSearchTool**\n- **Purpose**: Search file contents using regex patterns WITHOUT reading entire files\n- **This is your PRIMARY tool** for finding information in medium to large files\n- **Parameters**:\n  - `file_path`: File or directory to search\n  - `pattern`: Regex pattern (e.g., \"error|warning\", \"API.*authentication\")\n  - `output_mode`:\n    - \"files_only\" - Just list files containing matches\n    - \"matches_with_context\" - Show matching lines with surrounding context\n    - \"line_numbers\" - Show matches with line numbers only\n    - \"count_only\" - Count matches per file\n  - `context_lines`: Number of lines before/after match (default: 3)\n  - `case_sensitive`: false (default)\n  - `max_matches`: 100 (default)\n- **Returns**: Matching lines with context, file paths, or counts\n\n### 3. **ChunkedFileReader**\n- **Purpose**: Read large files in manageable chunks to avoid context overflow\n- **Use AFTER** Grep identifies relevant sections, or for systematic full-file reading\n- **Parameters**:\n  - `file_path`: Path to the file\n  - `chunk_number`: Which chunk to read (0-indexed)\n  - `chunk_size`: Lines per chunk (default: 500)\n  - `overlap_lines`: Overlap between chunks for context (default: 50)\n  - `include_line_numbers`: true (recommended)\n- **Returns**: Chunk content with metadata about position and whether more chunks exist\n\n## CRITICAL RULES - YOU MUST FOLLOW THESE\n\n### Rule 1: ALWAYS Check File Size First\n```\nCORRECT workflow:\n1. Use FileMetadataTool to get file info\n2. Read the processing_recommendation\n3. Follow the recommended strategy\n\nINCORRECT workflow:\n❌ Directly reading file without checking size\n❌ Attempting to read entire large files\n```\n\n### Rule 2: Use Grep for Search Tasks\n```\nFor ANY task involving finding specific information:\n✅ Use GrepSearchTool first\n✅ Use targeted regex patterns\n✅ Read only relevant sections afterward\n\nNever:\n❌ Read entire file to search for a keyword\n❌ Skip Grep and go straight to reading large files\n```\n\n### Rule 3: Respect File Size Thresholds\n\n**File Size Categories** (based on FileMetadataTool recommendations):\n\n- **TINY_FILE (<100 lines)**:\n  - Strategy: Read entire file at once\n  - Tool: ChunkedFileReader with chunk_number=0\n\n- **SMALL_FILE (100-500 lines)**:\n  - Strategy: Can read entire file\n  - Tool: ChunkedFileReader with chunk_number=0\n  - Alternative: Use Grep for specific searches\n\n- **MEDIUM_FILE (500-2,000 lines)**:\n  - Strategy: Grep first, then targeted reading\n  - Tool: GrepSearchTool → ChunkedFileReader for specific chunks\n  - Chunking: 2-4 chunks of 500 lines each if full read needed\n\n- **LARGE_FILE (2,000-5,000 lines)**:\n  - Strategy: **Required** - Use Grep to find sections\n  - Tool: GrepSearchTool (required) → ChunkedFileReader (targeted only)\n  - **DO NOT** read entire file\n\n- **VERY_LARGE_FILE (5,000-20,000 lines)**:\n  - Strategy: **Grep search ONLY** initially\n  - Tool: GrepSearchTool → ChunkedFileReader (very small targeted chunks)\n  - **NEVER** attempt full file read\n\n- **MASSIVE_FILE (>20,000 lines)**:\n  - Strategy: **CRITICAL** - Grep search exclusively\n  - Tool: GrepSearchTool only\n  - **ABSOLUTELY NO** full file reading\n  - Read specific chunks only after Grep identifies exact locations\n\n### Rule 4: Efficient Chunked Reading\n\nWhen using ChunkedFileReader:\n\n```python\n# Step 1: Read first chunk\nchunk_0 = ChunkedFileReader(file_path=\"file.txt\", chunk_number=0, chunk_size=500)\n\n# Step 2: Check if more chunks exist\nif chunk_0[\"has_more_chunks\"]:\n    # Read next chunk\n    chunk_1 = ChunkedFileReader(file_path=\"file.txt\", chunk_number=1, chunk_size=500)\n\n# Step 3: Continue until has_more_chunks is False\n```\n\n**Important**:\n- Use `overlap_lines=50` to maintain context between chunks\n- Always check `has_more_chunks` to know when to stop\n- Use `progress_percent` to track progress\n- Stop early if you found the information you need\n\n## Processing Workflows\n\n### Workflow 1: Search for Specific Information\n\n```\nTask: \"Find information about API authentication in config.json\"\n\nStep 1: Get file metadata\n  → FileMetadataTool(file_path=\"config.json\")\n  → Check total_lines and processing_recommendation\n\nStep 2: Search for relevant content\n  → GrepSearchTool(\n      file_path=\"config.json\",\n      pattern=\"API|authentication|auth\",\n      output_mode=\"matches_with_context\",\n      context_lines=5\n    )\n\nStep 3: Analyze results\n  → If matches found: Return information from Grep results\n  → If matches not clear: Read specific chunks containing matches\n\nStep 4: (Optional) Read specific sections\n  → Only if Grep results need more context\n  → ChunkedFileReader targeting specific line ranges\n```\n\n### Workflow 2: Summarize Large Document\n\n```\nTask: \"Summarize this 10,000 line document\"\n\nStep 1: Get file metadata\n  → FileMetadataTool(file_path=\"document.txt\")\n  → Note: File is VERY_LARGE_FILE\n\nStep 2: Extract structure\n  → GrepSearchTool(pattern=\"^#{1,3}\\s\", output_mode=\"matches_with_context\")\n    # This finds markdown headers\n  → GrepSearchTool(pattern=\"^(Introduction|Conclusion|Summary)\", ...)\n    # Find key sections\n\nStep 3: Read strategic sections\n  → ChunkedFileReader(chunk_number=0, chunk_size=200)  # Introduction\n  → ChunkedFileReader based on Grep findings for key sections\n  → ChunkedFileReader for last chunk  # Conclusion\n\nStep 4: Synthesize summary\n  → Combine information from strategic sections\n  → **DO NOT** read all 10,000 lines\n```\n\n### Workflow 3: Full File Analysis (Small Files Only)\n\n```\nTask: \"Analyze this 300-line file\"\n\nStep 1: Verify file size\n  → FileMetadataTool(file_path=\"file.txt\")\n  → Confirm: SMALL_FILE category\n\nStep 2: Read entire file\n  → ChunkedFileReader(chunk_number=0, chunk_size=500)\n  → Since file <500 lines, this reads entire file\n\nStep 3: Analyze content\n  → Process the content\n  → Extract information\n  → Provide analysis\n```\n\n### Workflow 4: Systematic Chunk-by-Chunk Processing\n\n```\nTask: \"Process all content in a 3,000 line file\"\n\nStep 1: Check file metadata\n  → FileMetadataTool(file_path=\"large_file.txt\")\n  → Note: LARGE_FILE - requires chunking\n\nStep 2: Process chunk by chunk\n  chunk_num = 0\n  while True:\n      chunk = ChunkedFileReader(\n          chunk_number=chunk_num,\n          chunk_size=500,\n          overlap_lines=50\n      )\n\n      # Process this chunk\n      analyze_chunk(chunk[\"content\"])\n\n      # Check if more chunks exist\n      if not chunk[\"has_more_chunks\"]:\n          break\n\n      chunk_num += 1\n\nStep 3: Combine results\n  → Synthesize findings from all chunks\n```\n\n## Response Formatting\n\nWhen presenting findings:\n\n### Always Include:\n1. **Source Citations**: Reference line numbers or chunk numbers\n   - Example: \"Found in lines 245-250\" or \"Chunk 3, lines 1500-2000\"\n\n2. **Progress Information**: For multi-chunk operations\n   - Example: \"Processed chunks 0-4 (50% complete)\"\n\n3. **Search Strategy Used**: Explain your approach\n   - Example: \"Used Grep to search for 'error' patterns, found 15 matches across 3 files\"\n\n4. **File Size Context**: Acknowledge file characteristics\n   - Example: \"This is a LARGE_FILE (3,500 lines), using chunked reading strategy\"\n\n### Response Template:\n\n```markdown\n## File Analysis Results\n\n**File**: [filename]\n**Size**: [X] lines, [Y] MB\n**Strategy Used**: [Grep search / Chunked reading / Full read]\n\n### Findings:\n\n[Your analysis here with specific citations]\n\n**Source**: Lines [X]-[Y] or Chunk [N]\n\n### Processing Details:\n- Chunks processed: [N] of [Total]\n- Search patterns used: [patterns]\n- Matches found: [count]\n\n[Additional details as needed]\n```\n\n## Error Handling\n\n### If File Not Found:\n```\nResponse: \"File not found at path: [path]. Please verify the file path and try again.\"\n```\n\n### If File Too Large to Read Fully:\n```\nResponse: \"This file is [X] lines (VERY_LARGE_FILE category). I cannot read it entirely.\nPlease provide specific search terms or questions, and I'll use Grep to find relevant sections.\"\n```\n\n### If Search Returns No Results:\n```\nResponse: \"No matches found for pattern '[pattern]' in [file].\nWould you like me to:\n1. Try a different search pattern\n2. Read the file in chunks to manually search\n3. Search a different file or directory\"\n```\n\n### If Encoding Issues:\n```\nResponse: \"Encountered encoding issues with [file]. Attempting alternative encodings...\"\n[Then try utf-8, latin-1, cp1252]\n```\n\n## Best Practices\n\n### DO:\n✅ Always use FileMetadataTool first\n✅ Use Grep for searching and finding information\n✅ Read files in chunks for files >500 lines\n✅ Include line numbers in citations\n✅ Stop reading early if you found what you need\n✅ Use regex patterns effectively in Grep\n✅ Check `has_more_chunks` to know when to stop\n✅ Use overlap_lines to maintain context\n\n### DON'T:\n❌ Read large files (>1000 lines) without chunking\n❌ Skip file metadata checks\n❌ Search by reading entire files instead of using Grep\n❌ Process all chunks when information is in first few\n❌ Ignore file size recommendations\n❌ Use patterns that are too broad (millions of matches)\n❌ Forget to include source citations\n\n## Example Interactions\n\n### Example 1: Finding Specific Information\n\n**User**: \"Find all TODO comments in the Python files in this directory\"\n\n**Your Response**:\n```\nI'll search for TODO comments in Python files.\n\nStep 1: Searching for TODO patterns...\n[Using GrepSearchTool with pattern=\"TODO|FIXME|XXX\"]\n\nResults: Found 12 TODO comments across 3 Python files:\n\n1. **app.py** (Line 145):\n   >>> 145 | # TODO: Add error handling for API timeouts\n\n2. **database.py** (Line 67):\n   >>> 67 | # FIXME: This query is inefficient, needs optimization\n\n[... additional matches ...]\n\nTotal: 12 TODO items found in 3 files.\nWould you like me to provide more details about any specific file?\n```\n\n### Example 2: Summarizing Large File\n\n**User**: \"Summarize this 5,000 line research paper\"\n\n**Your Response**:\n```\nI'll analyze this research paper efficiently.\n\nStep 1: Checking file size...\nFile: research_paper.txt (5,247 lines - VERY_LARGE_FILE category)\n\nStep 2: Using Grep to extract structure...\nFound:\n- Title and Abstract (lines 1-50)\n- 8 main sections with headers\n- Conclusion (lines 5,100-5,247)\n\nStep 3: Reading strategic sections...\n- Reading introduction chunk (lines 1-200)\n- Reading conclusion chunk (lines 5,100-5,247)\n- Extracting section headers and key findings\n\nSummary:\n[Provide summary based on strategic reading]\n\nNote: I used targeted chunk reading for this large file instead of reading all 5,247 lines.\nIf you need details about specific sections, I can search and read those targeted areas.\n```\n\n## Remember\n\nYour goal is to be **efficient** and **effective**:\n- Don't waste tokens reading irrelevant content\n- Use the right tool for each task\n- Follow file size recommendations strictly\n- Provide clear, cited responses\n- Be transparent about your search strategy\n\nYou are an expert at handling documents of ANY size by using the right combination of metadata checking, searching, and targeted reading.\n"
              },
              "tools": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Tools",
                "dynamic": false,
                "info": "These are the tools that the agent can use to help with tasks.",
                "input_types": [
                  "Tool"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "tools",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "verbose": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Verbose",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "verbose",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "Agent"
        },
        "dragging": false,
        "id": "Agent-zrOq9",
        "measured": {
          "height": 427,
          "width": 320
        },
        "position": {
          "x": 2920.20539058743,
          "y": -591.9516434429418
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "CustomComponent-RBhOR",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Executes shell commands, runs code in multiple languages, installs packages, and performs git operations with comprehensive security controls and verbose output.",
            "display_name": "Code Runner Tool",
            "documentation": "https://docs.langflow.org/components-custom",
            "edited": true,
            "field_order": [
              "command",
              "execution_mode",
              "package_manager",
              "working_directory",
              "environment_vars",
              "timeout",
              "security_mode",
              "allowed_commands",
              "blocked_commands",
              "output_format",
              "auto_install_packages",
              "verbose",
              "capture_realtime"
            ],
            "frozen": false,
            "icon": "terminal",
            "last_updated": "2025-11-10T22:21:40.993Z",
            "legacy": false,
            "lf_version": "1.6.0",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Toolset",
                "group_outputs": false,
                "hidden": null,
                "method": "to_toolkit",
                "name": "component_as_tool",
                "options": null,
                "required_inputs": null,
                "selected": "Tool",
                "tool_mode": true,
                "types": [
                  "Tool"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "allowed_commands": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "copy_field": false,
                "display_name": "Allowed Commands",
                "dynamic": false,
                "info": "Whitelist of allowed command patterns (one per line, used in Restricted mode)",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "allowed_commands",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "auto_install_packages": {
                "_input_type": "BoolInput",
                "advanced": false,
                "display_name": "Auto-install Packages",
                "dynamic": false,
                "info": "Automatically install missing packages when detected",
                "list": false,
                "list_add_label": "Add More",
                "name": "auto_install_packages",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "blocked_commands": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "copy_field": false,
                "display_name": "Blocked Commands",
                "dynamic": false,
                "info": "Blacklist of forbidden command patterns (one per line, used in Safe mode)",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "blocked_commands",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "rm -rf /\nformat\nmkfs\ndd if=/dev/zero\n:(){ :|:& };:"
              },
              "capture_realtime": {
                "_input_type": "BoolInput",
                "advanced": false,
                "display_name": "Real-time Output",
                "dynamic": false,
                "info": "Stream output as it's generated (for long-running commands)",
                "list": false,
                "list_add_label": "Add More",
                "name": "capture_realtime",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "\"\"\"\r\nCode Runner Tool Component for Langflow\r\n========================================\r\nA comprehensive shell command executor and code runner for Langflow agents.\r\nSupports shell commands, Python, Node.js, Git operations, and package management\r\nwith enhanced security controls and verbose output capabilities.\r\n\r\nAuthor: Claude\r\nVersion: 2.0.0\r\n\"\"\"\r\n\r\nimport subprocess\r\nimport sys\r\nimport json\r\nimport tempfile\r\nimport os\r\nfrom pathlib import Path\r\nfrom typing import Dict, List, Optional, Any\r\nimport time\r\nimport re\r\nimport shlex\r\nimport platform\r\n\r\nfrom langflow.custom import Component\r\nfrom langflow.io import (\r\n    DropdownInput,\r\n    MultilineInput,\r\n    MessageTextInput,\r\n    BoolInput,\r\n    IntInput,\r\n    Output,\r\n)\r\nfrom langflow.schema import Data\r\n\r\n\r\nclass CodeRunnerTool(Component):\r\n    \"\"\"\r\n    Advanced shell command executor and code runner for Langflow agents.\r\n    Supports shell commands, Python, Node.js, Git operations, and package management.\r\n    \"\"\"\r\n\r\n    display_name = \"Code Runner Tool\"\r\n    description = (\r\n        \"Executes shell commands, runs code in multiple languages, installs packages, \"\r\n        \"and performs git operations with comprehensive security controls and verbose output.\"\r\n    )\r\n    documentation = \"https://docs.langflow.org/components-custom\"\r\n    icon = \"terminal\"\r\n    name = \"CodeRunnerTool\"\r\n\r\n    # Tool capabilities description for agents\r\n    tool_description = \"\"\"\r\n    This tool can:\r\n    - Execute any shell/bash command (ls, pwd, cat, grep, etc.)\r\n    - Run Python code and scripts (with automatic package installation)\r\n    - Execute Node.js/JavaScript code\r\n    - Perform Git operations (clone, pull, push, commit, etc.)\r\n    - Install packages via pip, npm, yarn, apt-get, brew\r\n    - Create and manage files and directories\r\n    - Set environment variables\r\n    - Execute with timeout controls\r\n    - Provide detailed execution feedback and error messages\r\n    \"\"\"\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"command\",\r\n            display_name=\"Command/Code\",\r\n            info=\"Shell command or code to execute. Supports multi-line input for scripts.\",\r\n            tool_mode=True,  # Enable as agent tool\r\n            value=\"\"\r\n        ),\r\n        DropdownInput(\r\n            name=\"execution_mode\",\r\n            display_name=\"Execution Mode\",\r\n            info=\"How to interpret and execute the command\",\r\n            options=[\r\n                \"Auto-detect\",\r\n                \"Shell\",\r\n                \"Python\",\r\n                \"Node.js\",\r\n                \"Package Install\",\r\n                \"Git\"\r\n            ],\r\n            value=\"Auto-detect\"\r\n        ),\r\n        DropdownInput(\r\n            name=\"package_manager\",\r\n            display_name=\"Package Manager\",\r\n            info=\"Package manager to use for installations\",\r\n            options=[\r\n                \"Auto-detect\",\r\n                \"pip\",\r\n                \"pip3\",\r\n                \"npm\",\r\n                \"yarn\",\r\n                \"apt-get\",\r\n                \"apt\",\r\n                \"brew\",\r\n                \"dnf\",\r\n                \"yum\"\r\n            ],\r\n            value=\"Auto-detect\",\r\n            advanced=True\r\n        ),\r\n        MessageTextInput(\r\n            name=\"working_directory\",\r\n            display_name=\"Working Directory\",\r\n            info=\"Directory to execute the command in (defaults to current directory)\",\r\n            value=\"\",\r\n            advanced=True\r\n        ),\r\n        MultilineInput(\r\n            name=\"environment_vars\",\r\n            display_name=\"Environment Variables\",\r\n            info=\"Additional environment variables (KEY=VALUE format, one per line)\",\r\n            value=\"\",\r\n            advanced=True\r\n        ),\r\n        IntInput(\r\n            name=\"timeout\",\r\n            display_name=\"Timeout (seconds)\",\r\n            info=\"Maximum execution time in seconds (0 = no timeout)\",\r\n            value=30,\r\n            advanced=True\r\n        ),\r\n        DropdownInput(\r\n            name=\"security_mode\",\r\n            display_name=\"Security Mode\",\r\n            info=\"Security level for command execution\",\r\n            options=[\r\n                \"Safe (Recommended)\",\r\n                \"Unrestricted\",\r\n                \"Restricted\",\r\n                \"Read-only\"\r\n            ],\r\n            value=\"Safe (Recommended)\"\r\n        ),\r\n        MultilineInput(\r\n            name=\"allowed_commands\",\r\n            display_name=\"Allowed Commands\",\r\n            info=\"Whitelist of allowed command patterns (one per line, used in Restricted mode)\",\r\n            value=\"\",\r\n            advanced=True\r\n        ),\r\n        MultilineInput(\r\n            name=\"blocked_commands\",\r\n            display_name=\"Blocked Commands\",\r\n            info=\"Blacklist of forbidden command patterns (one per line, used in Safe mode)\",\r\n            value=\"rm -rf /\\nformat\\nmkfs\\ndd if=/dev/zero\\n:(){ :|:& };:\",\r\n            advanced=True\r\n        ),\r\n        DropdownInput(\r\n            name=\"output_format\",\r\n            display_name=\"Output Format\",\r\n            info=\"How to format the execution output\",\r\n            options=[\r\n                \"Full\",\r\n                \"Stdout only\",\r\n                \"Stderr only\",\r\n                \"Summary\",\r\n                \"JSON\"\r\n            ],\r\n            value=\"Full\"\r\n        ),\r\n        BoolInput(\r\n            name=\"auto_install_packages\",\r\n            display_name=\"Auto-install Packages\",\r\n            info=\"Automatically install missing packages when detected\",\r\n            value=False,\r\n            advanced=True\r\n        ),\r\n        BoolInput(\r\n            name=\"verbose\",\r\n            display_name=\"Verbose Output\",\r\n            info=\"Include detailed execution information and metrics\",\r\n            value=True\r\n        ),\r\n        BoolInput(\r\n            name=\"capture_realtime\",\r\n            display_name=\"Real-time Output\",\r\n            info=\"Stream output as it's generated (for long-running commands)\",\r\n            value=False,\r\n            advanced=True\r\n        )\r\n    ]\r\n\r\n    outputs = [\r\n        Output(\r\n            name=\"result\",\r\n            display_name=\"Execution Result\",\r\n            method=\"execute_command\"\r\n        )\r\n    ]\r\n\r\n    def build_config(self):\r\n        \"\"\"Dynamic configuration based on selected options.\"\"\"\r\n        config = super().build_config()\r\n\r\n        # Show/hide fields based on execution mode\r\n        if hasattr(self, 'execution_mode'):\r\n            if self.execution_mode == \"Package Install\":\r\n                config[\"package_manager\"][\"show\"] = True\r\n                config[\"auto_install_packages\"][\"show\"] = False\r\n            elif self.execution_mode in [\"Python\", \"Node.js\"]:\r\n                config[\"auto_install_packages\"][\"show\"] = True\r\n                config[\"package_manager\"][\"show\"] = False\r\n            else:\r\n                config[\"package_manager\"][\"show\"] = False\r\n                config[\"auto_install_packages\"][\"show\"] = False\r\n\r\n        # Show/hide security fields\r\n        if hasattr(self, 'security_mode'):\r\n            if self.security_mode == \"Restricted\":\r\n                config[\"allowed_commands\"][\"show\"] = True\r\n                config[\"blocked_commands\"][\"show\"] = False\r\n            elif self.security_mode == \"Safe (Recommended)\":\r\n                config[\"blocked_commands\"][\"show\"] = True\r\n                config[\"allowed_commands\"][\"show\"] = False\r\n            else:\r\n                config[\"allowed_commands\"][\"show\"] = False\r\n                config[\"blocked_commands\"][\"show\"] = False\r\n\r\n        return config\r\n\r\n    def execute_command(self) -> Data:\r\n        \"\"\"Execute the command with comprehensive error handling and output formatting.\"\"\"\r\n        start_time = time.time()\r\n\r\n        # Initialize result structure\r\n        result = {\r\n            \"success\": False,\r\n            \"command\": str(self.command).strip(),\r\n            \"mode\": self.execution_mode,\r\n            \"stdout\": \"\",\r\n            \"stderr\": \"\",\r\n            \"exit_code\": -1,\r\n            \"execution_time\": 0,\r\n            \"working_directory\": \"\",\r\n            \"environment\": {},\r\n            \"packages_installed\": [],\r\n            \"warnings\": [],\r\n            \"suggestions\": [],\r\n            \"error\": None\r\n        }\r\n\r\n        try:\r\n            # Validate and prepare command\r\n            command = self._prepare_command(result)\r\n            if not command:\r\n                return Data(data=result)\r\n\r\n            # Check security restrictions\r\n            if not self._check_security(command, result):\r\n                return Data(data=result)\r\n\r\n            # Detect and set execution mode\r\n            actual_mode = self._detect_execution_mode(command)\r\n            if self.execution_mode == \"Auto-detect\":\r\n                result[\"mode\"] = actual_mode\r\n\r\n            # Prepare environment\r\n            env = self._prepare_environment()\r\n            result[\"environment\"] = {k: v for k, v in env.items() if k not in os.environ}\r\n\r\n            # Set working directory\r\n            working_dir = self._get_working_directory()\r\n            result[\"working_directory\"] = str(working_dir)\r\n\r\n            # Execute based on mode\r\n            if result[\"mode\"] == \"Package Install\" or (\r\n                self.execution_mode == \"Package Install\"\r\n            ):\r\n                self._execute_package_install(command, result, env)\r\n            elif result[\"mode\"] == \"Python\" or (\r\n                self.execution_mode == \"Python\"\r\n            ):\r\n                self._execute_python(command, result, env, working_dir)\r\n            elif result[\"mode\"] == \"Node.js\" or (\r\n                self.execution_mode == \"Node.js\"\r\n            ):\r\n                self._execute_nodejs(command, result, env, working_dir)\r\n            elif result[\"mode\"] == \"Git\" or (\r\n                self.execution_mode == \"Git\"\r\n            ):\r\n                self._execute_git(command, result, env, working_dir)\r\n            else:\r\n                # Default shell execution\r\n                self._execute_shell(command, result, env, working_dir)\r\n\r\n        except Exception as e:\r\n            result[\"success\"] = False\r\n            result[\"error\"] = str(e)\r\n            result[\"stderr\"] = f\"Execution error: {str(e)}\"\r\n            result[\"suggestions\"].append(\r\n                \"Check command syntax and ensure required tools are installed\"\r\n            )\r\n\r\n        finally:\r\n            result[\"execution_time\"] = round(time.time() - start_time, 3)\r\n\r\n            # Format output based on selection\r\n            formatted_result = self._format_output(result)\r\n\r\n            # Update component status\r\n            if result[\"success\"]:\r\n                self.status = f\"✓ Command executed successfully in {result['execution_time']}s\"\r\n            else:\r\n                self.status = f\"✗ Command failed: {result.get('error', 'Unknown error')}\"\r\n\r\n            return Data(data=formatted_result)\r\n\r\n    def _prepare_command(self, result: Dict) -> Optional[str]:\r\n        \"\"\"Prepare and validate the command.\"\"\"\r\n        command = str(self.command).strip()\r\n\r\n        if not command:\r\n            result[\"error\"] = \"No command provided\"\r\n            result[\"suggestions\"].append(\"Please provide a command to execute\")\r\n            return None\r\n\r\n        # Add command history tracking\r\n        if self.verbose:\r\n            result[\"warnings\"].append(f\"Executing: {command[:100]}...\" if len(command) > 100 else f\"Executing: {command}\")\r\n\r\n        return command\r\n\r\n    def _check_security(self, command: str, result: Dict) -> bool:\r\n        \"\"\"Check security restrictions based on mode.\"\"\"\r\n        mode = self.security_mode\r\n\r\n        if mode == \"Read-only\":\r\n            # Only allow read operations\r\n            write_patterns = [\r\n                'write', 'rm', 'del', 'delete', 'mv', 'move', 'cp', 'copy',\r\n                'mkdir', 'touch', 'chmod', 'chown', '>', '>>', 'tee',\r\n                'install', 'uninstall', 'update', 'upgrade'\r\n            ]\r\n            if any(pattern in command.lower() for pattern in write_patterns):\r\n                result[\"error\"] = \"Write operations not allowed in Read-only mode\"\r\n                result[\"suggestions\"].append(\"Switch to a different security mode to perform write operations\")\r\n                return False\r\n\r\n        elif mode == \"Restricted\":\r\n            # Check against whitelist\r\n            allowed = [line.strip() for line in str(self.allowed_commands or \"\").splitlines() if line.strip()]\r\n            if not allowed:\r\n                result[\"error\"] = \"No allowed commands defined in Restricted mode\"\r\n                result[\"suggestions\"].append(\"Add allowed command patterns or switch security mode\")\r\n                return False\r\n\r\n            if not any(re.search(pattern, command) for pattern in allowed):\r\n                result[\"error\"] = f\"Command not in allowed list for Restricted mode\"\r\n                result[\"suggestions\"].append(f\"Allowed patterns: {', '.join(allowed[:5])}\")\r\n                return False\r\n\r\n        elif mode == \"Safe (Recommended)\":\r\n            # Check against blacklist\r\n            blocked = [line.strip() for line in str(self.blocked_commands or \"\").splitlines() if line.strip()]\r\n            for pattern in blocked:\r\n                if pattern in command:\r\n                    result[\"error\"] = f\"Command contains blocked pattern: {pattern}\"\r\n                    result[\"warnings\"].append(\"This command is potentially dangerous\")\r\n                    result[\"suggestions\"].append(\"Review the command for safety or switch to Unrestricted mode\")\r\n                    return False\r\n\r\n        # Check for additional dangerous patterns in Safe mode\r\n        if mode == \"Safe (Recommended)\":\r\n            dangerous_patterns = [\r\n                (r'rm\\s+-rf\\s+/', \"Attempting to delete root directory\"),\r\n                (r':(){ :|:& };:', \"Fork bomb detected\"),\r\n                (r'dd\\s+if=/dev/(zero|random)', \"Potentially destructive disk operation\"),\r\n                (r'mkfs', \"Filesystem format command detected\"),\r\n                (r'format\\s+[cC]:', \"Windows format command detected\")\r\n            ]\r\n\r\n            for pattern, warning in dangerous_patterns:\r\n                if re.search(pattern, command):\r\n                    result[\"error\"] = warning\r\n                    result[\"warnings\"].append(\"This operation could damage your system\")\r\n                    return False\r\n\r\n        return True\r\n\r\n    def _detect_execution_mode(self, command: str) -> str:\r\n        \"\"\"Auto-detect the execution mode from command.\"\"\"\r\n        command_lower = command.lower()\r\n\r\n        # Check for package installation commands\r\n        package_managers = ['pip', 'npm', 'yarn', 'apt-get', 'apt', 'brew', 'dnf', 'yum']\r\n        for pm in package_managers:\r\n            if command_lower.startswith(pm + ' install') or command_lower.startswith(pm + ' add'):\r\n                return \"Package Install\"\r\n\r\n        # Check for git commands\r\n        if command_lower.startswith('git '):\r\n            return \"Git\"\r\n\r\n        # Check for Python\r\n        if command_lower.startswith('python') or command_lower.startswith('py '):\r\n            return \"Python\"\r\n\r\n        # Check for Node.js\r\n        if command_lower.startswith('node ') or command_lower.startswith('npm ') or command_lower.startswith('yarn '):\r\n            return \"Node.js\"\r\n\r\n        # Check for inline Python code\r\n        if 'import ' in command or 'def ' in command or 'print(' in command:\r\n            return \"Python\"\r\n\r\n        # Check for inline JavaScript code\r\n        if 'const ' in command or 'let ' in command or 'console.log' in command or 'require(' in command:\r\n            return \"Node.js\"\r\n\r\n        return \"Shell\"\r\n\r\n    def _prepare_environment(self) -> Dict[str, str]:\r\n        \"\"\"Prepare environment variables.\"\"\"\r\n        env = os.environ.copy()\r\n\r\n        # Add custom environment variables\r\n        if self.environment_vars:\r\n            for line in str(self.environment_vars).splitlines():\r\n                line = line.strip()\r\n                if '=' in line:\r\n                    key, value = line.split('=', 1)\r\n                    env[key.strip()] = value.strip()\r\n\r\n        # Add tool-specific environment variables\r\n        env['LANGFLOW_TOOL'] = 'CodeRunnerTool'\r\n        env['PYTHONUNBUFFERED'] = '1'  # For real-time Python output\r\n\r\n        return env\r\n\r\n    def _get_working_directory(self) -> Path:\r\n        \"\"\"Get and validate working directory.\"\"\"\r\n        if self.working_directory:\r\n            work_dir = Path(self.working_directory).expanduser()\r\n            if not work_dir.exists():\r\n                work_dir.mkdir(parents=True, exist_ok=True)\r\n            return work_dir\r\n        return Path.cwd()\r\n\r\n    def _execute_shell(self, command: str, result: Dict, env: Dict, working_dir: Path):\r\n        \"\"\"Execute shell command.\"\"\"\r\n        try:\r\n            # Prepare shell command\r\n            if platform.system() == \"Windows\":\r\n                shell_cmd = command\r\n                shell = True\r\n            else:\r\n                shell_cmd = command\r\n                shell = True\r\n\r\n            # Execute with timeout\r\n            process = subprocess.run(\r\n                shell_cmd,\r\n                shell=shell,\r\n                capture_output=True,\r\n                text=True,\r\n                timeout=self.timeout if self.timeout > 0 else None,\r\n                env=env,\r\n                cwd=str(working_dir)\r\n            )\r\n\r\n            result[\"stdout\"] = process.stdout\r\n            result[\"stderr\"] = process.stderr\r\n            result[\"exit_code\"] = process.returncode\r\n            result[\"success\"] = (process.returncode == 0)\r\n\r\n            # Add suggestions for common errors\r\n            if process.returncode != 0:\r\n                self._add_error_suggestions(command, process.stderr, result)\r\n\r\n        except subprocess.TimeoutExpired:\r\n            result[\"error\"] = f\"Command timed out after {self.timeout} seconds\"\r\n            result[\"suggestions\"].append(\"Increase timeout or optimize the command\")\r\n        except Exception as e:\r\n            result[\"error\"] = str(e)\r\n            result[\"stderr\"] = str(e)\r\n\r\n    def _execute_python(self, command: str, result: Dict, env: Dict, working_dir: Path):\r\n        \"\"\"Execute Python code or script.\"\"\"\r\n        # Check if it's a file execution or inline code\r\n        if command.startswith('python ') or command.startswith('python3 '):\r\n            # File execution\r\n            self._execute_shell(command, result, env, working_dir)\r\n        else:\r\n            # Inline code execution\r\n            try:\r\n                # Check for missing packages\r\n                if self.auto_install_packages:\r\n                    missing_packages = self._detect_missing_python_packages(command)\r\n                    if missing_packages:\r\n                        result[\"warnings\"].append(f\"Installing missing packages: {', '.join(missing_packages)}\")\r\n                        for package in missing_packages:\r\n                            self._install_python_package(package, result)\r\n\r\n                # Create temporary Python file\r\n                with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, dir=working_dir) as f:\r\n                    f.write(command)\r\n                    temp_file = f.name\r\n\r\n                try:\r\n                    # Execute Python file\r\n                    python_cmd = f\"{sys.executable} {temp_file}\"\r\n                    self._execute_shell(python_cmd, result, env, working_dir)\r\n                finally:\r\n                    # Clean up\r\n                    os.unlink(temp_file)\r\n\r\n            except Exception as e:\r\n                result[\"error\"] = f\"Python execution error: {str(e)}\"\r\n                result[\"stderr\"] = str(e)\r\n\r\n    def _execute_nodejs(self, command: str, result: Dict, env: Dict, working_dir: Path):\r\n        \"\"\"Execute Node.js/JavaScript code.\"\"\"\r\n        # Check if it's a file execution or inline code\r\n        if command.startswith('node ') or command.startswith('npm ') or command.startswith('yarn '):\r\n            # Direct command execution\r\n            self._execute_shell(command, result, env, working_dir)\r\n        else:\r\n            # Inline code execution\r\n            try:\r\n                # Check for missing packages\r\n                if self.auto_install_packages:\r\n                    missing_packages = self._detect_missing_node_packages(command)\r\n                    if missing_packages:\r\n                        result[\"warnings\"].append(f\"Installing missing packages: {', '.join(missing_packages)}\")\r\n                        for package in missing_packages:\r\n                            self._install_node_package(package, result, working_dir)\r\n\r\n                # Create temporary JS file\r\n                with tempfile.NamedTemporaryFile(mode='w', suffix='.js', delete=False, dir=working_dir) as f:\r\n                    f.write(command)\r\n                    temp_file = f.name\r\n\r\n                try:\r\n                    # Execute Node.js file\r\n                    node_cmd = f\"node {temp_file}\"\r\n                    self._execute_shell(node_cmd, result, env, working_dir)\r\n                finally:\r\n                    # Clean up\r\n                    os.unlink(temp_file)\r\n\r\n            except Exception as e:\r\n                result[\"error\"] = f\"Node.js execution error: {str(e)}\"\r\n                result[\"stderr\"] = str(e)\r\n\r\n    def _execute_git(self, command: str, result: Dict, env: Dict, working_dir: Path):\r\n        \"\"\"Execute Git commands with enhanced error handling.\"\"\"\r\n        # Add git-specific environment variables\r\n        env['GIT_TERMINAL_PROMPT'] = '0'  # Disable password prompts\r\n\r\n        # Execute git command\r\n        self._execute_shell(command, result, env, working_dir)\r\n\r\n        # Add git-specific suggestions for common errors\r\n        if not result[\"success\"]:\r\n            stderr_lower = result[\"stderr\"].lower()\r\n            if \"not a git repository\" in stderr_lower:\r\n                result[\"suggestions\"].append(\"Initialize a git repository with 'git init' or navigate to an existing repository\")\r\n            elif \"permission denied\" in stderr_lower:\r\n                result[\"suggestions\"].append(\"Check repository permissions or authentication credentials\")\r\n            elif \"could not resolve host\" in stderr_lower:\r\n                result[\"suggestions\"].append(\"Check network connection and repository URL\")\r\n\r\n    def _execute_package_install(self, command: str, result: Dict, env: Dict):\r\n        \"\"\"Execute package installation commands.\"\"\"\r\n        # Detect package manager\r\n        pm = self._detect_package_manager(command)\r\n\r\n        if pm == \"Auto-detect\":\r\n            # Try to detect from command\r\n            if 'pip' in command:\r\n                pm = 'pip'\r\n            elif 'npm' in command:\r\n                pm = 'npm'\r\n            elif 'yarn' in command:\r\n                pm = 'yarn'\r\n            elif 'apt' in command:\r\n                pm = 'apt-get'\r\n            elif 'brew' in command:\r\n                pm = 'brew'\r\n            else:\r\n                result[\"error\"] = \"Could not detect package manager\"\r\n                result[\"suggestions\"].append(\"Specify the package manager explicitly\")\r\n                return\r\n\r\n        # Execute installation\r\n        self._execute_shell(command, result, env, Path.cwd())\r\n\r\n        # Track installed packages\r\n        if result[\"success\"]:\r\n            packages = self._extract_package_names(command, pm)\r\n            result[\"packages_installed\"] = packages\r\n            if packages:\r\n                result[\"warnings\"].append(f\"Successfully installed: {', '.join(packages)}\")\r\n\r\n    def _detect_package_manager(self, command: str) -> str:\r\n        \"\"\"Detect which package manager to use.\"\"\"\r\n        if self.package_manager != \"Auto-detect\":\r\n            return self.package_manager\r\n\r\n        command_lower = command.lower()\r\n        if 'pip' in command_lower:\r\n            return 'pip'\r\n        elif 'npm' in command_lower:\r\n            return 'npm'\r\n        elif 'yarn' in command_lower:\r\n            return 'yarn'\r\n        elif 'apt-get' in command_lower or 'apt ' in command_lower:\r\n            return 'apt-get'\r\n        elif 'brew' in command_lower:\r\n            return 'brew'\r\n        elif 'dnf' in command_lower:\r\n            return 'dnf'\r\n        elif 'yum' in command_lower:\r\n            return 'yum'\r\n\r\n        return \"Auto-detect\"\r\n\r\n    def _detect_missing_python_packages(self, code: str) -> List[str]:\r\n        \"\"\"Detect missing Python packages from import statements.\"\"\"\r\n        missing = []\r\n        import_pattern = r'(?:from|import)\\s+(\\w+)'\r\n\r\n        for match in re.finditer(import_pattern, code):\r\n            package = match.group(1)\r\n            # Check if package is importable\r\n            try:\r\n                __import__(package)\r\n            except ImportError:\r\n                # Map common aliases to actual package names\r\n                package_map = {\r\n                    'cv2': 'opencv-python',\r\n                    'sklearn': 'scikit-learn',\r\n                    'PIL': 'Pillow',\r\n                    'yaml': 'PyYAML'\r\n                }\r\n                actual_package = package_map.get(package, package)\r\n                if actual_package not in missing:\r\n                    missing.append(actual_package)\r\n\r\n        return missing\r\n\r\n    def _detect_missing_node_packages(self, code: str) -> List[str]:\r\n        \"\"\"Detect missing Node.js packages from require statements.\"\"\"\r\n        missing = []\r\n        require_pattern = r'require\\([\\'\"](\\w+)[\\'\"]\\)'\r\n        import_pattern = r'import\\s+.*\\s+from\\s+[\\'\"](\\w+)[\\'\"]'\r\n\r\n        patterns = [require_pattern, import_pattern]\r\n        for pattern in patterns:\r\n            for match in re.finditer(pattern, code):\r\n                package = match.group(1)\r\n                # Check if package exists (simplified check)\r\n                check_cmd = f\"npm list {package} --depth=0\"\r\n                result = subprocess.run(check_cmd, shell=True, capture_output=True)\r\n                if result.returncode != 0 and package not in missing:\r\n                    missing.append(package)\r\n\r\n        return missing\r\n\r\n    def _install_python_package(self, package: str, result: Dict):\r\n        \"\"\"Install a Python package.\"\"\"\r\n        cmd = f\"{sys.executable} -m pip install {package}\"\r\n        process = subprocess.run(cmd, shell=True, capture_output=True, text=True)\r\n        if process.returncode == 0:\r\n            result[\"packages_installed\"].append(package)\r\n        else:\r\n            result[\"warnings\"].append(f\"Failed to install {package}: {process.stderr}\")\r\n\r\n    def _install_node_package(self, package: str, result: Dict, working_dir: Path):\r\n        \"\"\"Install a Node.js package.\"\"\"\r\n        # Check if package.json exists\r\n        package_json = working_dir / \"package.json\"\r\n        if not package_json.exists():\r\n            # Initialize npm if needed\r\n            init_cmd = \"npm init -y\"\r\n            subprocess.run(init_cmd, shell=True, capture_output=True, cwd=str(working_dir))\r\n\r\n        cmd = f\"npm install {package}\"\r\n        process = subprocess.run(cmd, shell=True, capture_output=True, text=True, cwd=str(working_dir))\r\n        if process.returncode == 0:\r\n            result[\"packages_installed\"].append(package)\r\n        else:\r\n            result[\"warnings\"].append(f\"Failed to install {package}: {process.stderr}\")\r\n\r\n    def _extract_package_names(self, command: str, package_manager: str) -> List[str]:\r\n        \"\"\"Extract package names from install command.\"\"\"\r\n        packages = []\r\n\r\n        # Remove the package manager and install command\r\n        patterns = [\r\n            f'{package_manager} install',\r\n            f'{package_manager} add',\r\n            f'{package_manager} i'\r\n        ]\r\n\r\n        clean_cmd = command\r\n        for pattern in patterns:\r\n            clean_cmd = clean_cmd.replace(pattern, '').strip()\r\n\r\n        # Split by spaces and filter flags\r\n        for item in clean_cmd.split():\r\n            if not item.startswith('-') and item not in ['sudo', 'global', '-g']:\r\n                packages.append(item)\r\n\r\n        return packages\r\n\r\n    def _add_error_suggestions(self, command: str, stderr: str, result: Dict):\r\n        \"\"\"Add helpful suggestions based on error patterns.\"\"\"\r\n        stderr_lower = stderr.lower()\r\n\r\n        # Command not found\r\n        if \"command not found\" in stderr_lower or \"not recognized\" in stderr_lower:\r\n            cmd_parts = command.split()\r\n            if cmd_parts:\r\n                tool = cmd_parts[0]\r\n                result[\"suggestions\"].append(f\"The command '{tool}' is not installed or not in PATH\")\r\n\r\n                # Suggest installation commands\r\n                install_suggestions = {\r\n                    'git': 'Install git: apt-get install git (Linux) or brew install git (Mac)',\r\n                    'python': 'Install Python from python.org',\r\n                    'node': 'Install Node.js from nodejs.org',\r\n                    'npm': 'Comes with Node.js installation',\r\n                    'pip': 'Install pip: python -m ensurepip',\r\n                }\r\n\r\n                if tool in install_suggestions:\r\n                    result[\"suggestions\"].append(install_suggestions[tool])\r\n\r\n        # Permission denied\r\n        elif \"permission denied\" in stderr_lower:\r\n            result[\"suggestions\"].append(\"Try running with appropriate permissions or in a different directory\")\r\n            if platform.system() != \"Windows\":\r\n                result[\"suggestions\"].append(\"On Unix systems, you might need to use 'sudo' for system operations\")\r\n\r\n        # File not found\r\n        elif \"no such file\" in stderr_lower or \"cannot find\" in stderr_lower:\r\n            result[\"suggestions\"].append(\"Check that the file or directory exists and the path is correct\")\r\n            result[\"suggestions\"].append(f\"Current working directory: {Path.cwd()}\")\r\n\r\n        # Network errors\r\n        elif \"could not resolve\" in stderr_lower or \"connection\" in stderr_lower:\r\n            result[\"suggestions\"].append(\"Check your network connection\")\r\n            result[\"suggestions\"].append(\"Verify that the URL or hostname is correct\")\r\n\r\n    def _format_output(self, result: Dict) -> Any:\r\n        \"\"\"Format the output based on selected format.\"\"\"\r\n        format_type = self.output_format\r\n\r\n        if format_type == \"JSON\":\r\n            return result\r\n\r\n        elif format_type == \"Summary\":\r\n            summary = []\r\n            summary.append(f\"{'✓' if result['success'] else '✗'} Command: {result['command'][:50]}...\")\r\n            summary.append(f\"Mode: {result['mode']}\")\r\n            summary.append(f\"Exit Code: {result['exit_code']}\")\r\n            summary.append(f\"Execution Time: {result['execution_time']}s\")\r\n\r\n            if result.get('error'):\r\n                summary.append(f\"Error: {result['error']}\")\r\n\r\n            if result.get('packages_installed'):\r\n                summary.append(f\"Packages Installed: {', '.join(result['packages_installed'])}\")\r\n\r\n            if result.get('suggestions'):\r\n                summary.append(\"\\nSuggestions:\")\r\n                for suggestion in result['suggestions']:\r\n                    summary.append(f\"  • {suggestion}\")\r\n\r\n            return {\"output\": \"\\n\".join(summary)}\r\n\r\n        elif format_type == \"Stdout only\":\r\n            return {\"output\": result.get('stdout', '')}\r\n\r\n        elif format_type == \"Stderr only\":\r\n            return {\"output\": result.get('stderr', '')}\r\n\r\n        else:  # Full output\r\n            output_parts = []\r\n\r\n            if self.verbose:\r\n                # Add execution details\r\n                output_parts.append(\"=\" * 60)\r\n                output_parts.append(f\"EXECUTION DETAILS\")\r\n                output_parts.append(\"=\" * 60)\r\n                output_parts.append(f\"Command: {result['command']}\")\r\n                output_parts.append(f\"Mode: {result['mode']}\")\r\n                output_parts.append(f\"Working Directory: {result['working_directory']}\")\r\n                output_parts.append(f\"Timeout: {self.timeout}s\")\r\n                output_parts.append(f\"Security Mode: {self.security_mode}\")\r\n\r\n                if result['environment']:\r\n                    output_parts.append(f\"Environment Variables: {json.dumps(result['environment'], indent=2)}\")\r\n\r\n                output_parts.append(\"=\" * 60)\r\n                output_parts.append(\"\")\r\n\r\n            # Add warnings\r\n            if result.get('warnings'):\r\n                output_parts.append(\"⚠ WARNINGS:\")\r\n                for warning in result['warnings']:\r\n                    output_parts.append(f\"  {warning}\")\r\n                output_parts.append(\"\")\r\n\r\n            # Add stdout\r\n            if result.get('stdout'):\r\n                output_parts.append(\"STDOUT:\")\r\n                output_parts.append(\"-\" * 40)\r\n                output_parts.append(result['stdout'])\r\n                output_parts.append(\"\")\r\n\r\n            # Add stderr\r\n            if result.get('stderr'):\r\n                output_parts.append(\"STDERR:\")\r\n                output_parts.append(\"-\" * 40)\r\n                output_parts.append(result['stderr'])\r\n                output_parts.append(\"\")\r\n\r\n            # Add error if present\r\n            if result.get('error'):\r\n                output_parts.append(\"ERROR:\")\r\n                output_parts.append(\"-\" * 40)\r\n                output_parts.append(result['error'])\r\n                output_parts.append(\"\")\r\n\r\n            # Add execution metrics\r\n            if self.verbose:\r\n                output_parts.append(\"EXECUTION METRICS:\")\r\n                output_parts.append(\"-\" * 40)\r\n                output_parts.append(f\"Exit Code: {result['exit_code']}\")\r\n                output_parts.append(f\"Success: {result['success']}\")\r\n                output_parts.append(f\"Execution Time: {result['execution_time']} seconds\")\r\n\r\n                if result.get('packages_installed'):\r\n                    output_parts.append(f\"Packages Installed: {', '.join(result['packages_installed'])}\")\r\n                output_parts.append(\"\")\r\n\r\n            # Add suggestions\r\n            if result.get('suggestions'):\r\n                output_parts.append(\"💡 SUGGESTIONS:\")\r\n                for suggestion in result['suggestions']:\r\n                    output_parts.append(f\"  • {suggestion}\")\r\n\r\n            return {\"output\": \"\\n\".join(output_parts)}"
              },
              "command": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Command/Code",
                "dynamic": false,
                "info": "Shell command or code to execute. Supports multi-line input for scripts.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "command",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "environment_vars": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "copy_field": false,
                "display_name": "Environment Variables",
                "dynamic": false,
                "info": "Additional environment variables (KEY=VALUE format, one per line)",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "environment_vars",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "execution_mode": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Execution Mode",
                "dynamic": false,
                "external_options": {},
                "info": "How to interpret and execute the command",
                "name": "execution_mode",
                "options": [
                  "Auto-detect",
                  "Shell",
                  "Python",
                  "Node.js",
                  "Package Install",
                  "Git"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Auto-detect"
              },
              "output_format": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Output Format",
                "dynamic": false,
                "external_options": {},
                "info": "How to format the execution output",
                "name": "output_format",
                "options": [
                  "Full",
                  "Stdout only",
                  "Stderr only",
                  "Summary",
                  "JSON"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Full"
              },
              "package_manager": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Package Manager",
                "dynamic": false,
                "external_options": {},
                "info": "Package manager to use for installations",
                "name": "package_manager",
                "options": [
                  "Auto-detect",
                  "pip",
                  "pip3",
                  "npm",
                  "yarn",
                  "apt-get",
                  "apt",
                  "brew",
                  "dnf",
                  "yum"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Auto-detect"
              },
              "security_mode": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Security Mode",
                "dynamic": false,
                "external_options": {},
                "info": "Security level for command execution",
                "name": "security_mode",
                "options": [
                  "Safe (Recommended)",
                  "Unrestricted",
                  "Restricted",
                  "Read-only"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Unrestricted"
              },
              "timeout": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Timeout (seconds)",
                "dynamic": false,
                "info": "Maximum execution time in seconds (0 = no timeout)",
                "list": false,
                "list_add_label": "Add More",
                "name": "timeout",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 30
              },
              "tools_metadata": {
                "_input_type": "ToolsInput",
                "advanced": false,
                "display_name": "Actions",
                "dynamic": false,
                "info": "Modify tool names and descriptions to help agents understand when to use each tool.",
                "is_list": true,
                "list_add_label": "Add More",
                "name": "tools_metadata",
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "tools",
                "value": [
                  {
                    "args": {
                      "command": {
                        "default": "",
                        "description": "Shell command or code to execute. Supports multi-line input for scripts.",
                        "title": "Command",
                        "type": "string"
                      }
                    },
                    "description": "Executes shell commands, runs code in multiple languages, installs packages, and performs git operations with comprehensive security controls and verbose output.",
                    "display_description": "Executes shell commands, runs code in multiple languages, installs packages, and performs git operations with comprehensive security controls and verbose output.",
                    "display_name": "execute_command",
                    "name": "execute_command",
                    "readonly": false,
                    "status": true,
                    "tags": [
                      "execute_command"
                    ]
                  }
                ]
              },
              "verbose": {
                "_input_type": "BoolInput",
                "advanced": false,
                "display_name": "Verbose Output",
                "dynamic": false,
                "info": "Include detailed execution information and metrics",
                "list": false,
                "list_add_label": "Add More",
                "name": "verbose",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "working_directory": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Working Directory",
                "dynamic": false,
                "info": "Directory to execute the command in (defaults to current directory)",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "working_directory",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": true
          },
          "showNode": true,
          "type": "CodeRunnerTool"
        },
        "dragging": false,
        "id": "CustomComponent-RBhOR",
        "measured": {
          "height": 621,
          "width": 320
        },
        "position": {
          "x": 2225.5119374273377,
          "y": 456.37830068023595
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "CustomComponent-frHjb",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get file size, type, and metadata to plan processing strategy",
            "display_name": "File Metadata Tool",
            "documentation": "",
            "edited": true,
            "field_order": [
              "file_path",
              "include_recommendations",
              "count_lines"
            ],
            "frozen": false,
            "icon": "info",
            "last_updated": "2025-11-10T20:26:10.240Z",
            "legacy": false,
            "lf_version": "1.6.0",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Toolset",
                "group_outputs": false,
                "hidden": null,
                "method": "to_toolkit",
                "name": "component_as_tool",
                "options": null,
                "required_inputs": null,
                "selected": "Tool",
                "tool_mode": true,
                "types": [
                  "Tool"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "\"\"\"\nFile Metadata Tool for Langflow\nGet file information before reading to make intelligent decisions about processing strategy\n\"\"\"\n\nfrom langflow.custom import Component\nfrom langflow.io import StrInput, BoolInput, Output\nfrom langflow.schema import Data\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Dict, Any, List\nimport mimetypes\n\n\nclass FileMetadataTool(Component):\n    \"\"\"\n    Get comprehensive metadata about files without reading their contents.\n    Essential for making intelligent decisions about how to process files.\n    \"\"\"\n\n    display_name = \"File Metadata Tool\"\n    description = \"Get file size, type, and metadata to plan processing strategy\"\n    icon = \"info\"\n    name = \"FileMetadataTool\"\n\n    inputs = [\n        StrInput(\n            name=\"file_path\",\n            display_name=\"File Path\",\n            info=\"Path to file or directory\",\n            required=True,\n            tool_mode=True\n        ),\n        BoolInput(\n            name=\"include_recommendations\",\n            display_name=\"Include Processing Recommendations\",\n            value=True,\n            info=\"Include suggested processing strategies based on file size\",\n            advanced=True\n        ),\n        BoolInput(\n            name=\"count_lines\",\n            display_name=\"Count Lines\",\n            value=True,\n            info=\"Count total lines in text files (may be slow for large files)\",\n            advanced=True\n        )\n    ]\n\n    outputs = [\n        Output(\n            name=\"metadata\",\n            display_name=\"File Metadata\",\n            method=\"get_metadata\"\n        ),\n        Output(\n            name=\"recommendation\",\n            display_name=\"Processing Recommendation\",\n            method=\"get_recommendation\"\n        )\n    ]\n\n    def get_metadata(self) -> Data:\n        \"\"\"\n        Get comprehensive file metadata\n\n        Returns:\n            Data object with file metadata\n        \"\"\"\n        try:\n            path = Path(self.file_path)\n\n            if not path.exists():\n                return Data(data={\n                    \"exists\": False,\n                    \"error\": f\"Path does not exist: {self.file_path}\",\n                    \"path\": str(path)\n                })\n\n            # Get basic stats\n            stat = path.stat()\n            is_file = path.is_file()\n            is_dir = path.is_dir()\n\n            # Build metadata\n            metadata = {\n                \"exists\": True,\n                \"path\": str(path.absolute()),\n                \"name\": path.name,\n                \"parent\": str(path.parent),\n                \"is_file\": is_file,\n                \"is_directory\": is_dir,\n                \"size_bytes\": stat.st_size,\n                \"size_kb\": round(stat.st_size / 1024, 2),\n                \"size_mb\": round(stat.st_size / (1024 * 1024), 2),\n                \"created_timestamp\": stat.st_ctime,\n                \"modified_timestamp\": stat.st_mtime,\n                \"accessed_timestamp\": stat.st_atime,\n                \"created_date\": datetime.fromtimestamp(stat.st_ctime).isoformat(),\n                \"modified_date\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n                \"accessed_date\": datetime.fromtimestamp(stat.st_atime).isoformat()\n            }\n\n            if is_file:\n                # Add file-specific metadata\n                mime_type, _ = mimetypes.guess_type(str(path))\n                metadata.update({\n                    \"extension\": path.suffix,\n                    \"mime_type\": mime_type or \"unknown\",\n                    \"is_text_file\": self._is_text_file(path, mime_type)\n                })\n\n                # Count lines for text files\n                if metadata[\"is_text_file\"] and self.count_lines:\n                    line_info = self._count_lines_safe(path)\n                    metadata.update(line_info)\n\n                # Add processing recommendations\n                if self.include_recommendations:\n                    metadata[\"processing_recommendation\"] = self._get_processing_strategy(metadata)\n\n            elif is_dir:\n                # Add directory-specific metadata\n                dir_info = self._get_directory_info(path)\n                metadata.update(dir_info)\n\n            self.status = f\"Retrieved metadata for: {path.name}\"\n            return Data(data=metadata)\n\n        except Exception as e:\n            self.log(f\"Error getting metadata: {str(e)}\")\n            return Data(data={\n                \"error\": str(e),\n                \"path\": self.file_path,\n                \"exists\": False\n            })\n\n    def _is_text_file(self, path: Path, mime_type: str) -> bool:\n        \"\"\"\n        Determine if file is likely a text file\n\n        Args:\n            path: File path\n            mime_type: MIME type\n\n        Returns:\n            True if likely a text file\n        \"\"\"\n        # Check by MIME type\n        if mime_type and mime_type.startswith('text/'):\n            return True\n\n        # Check by extension\n        text_extensions = {\n            '.txt', '.md', '.json', '.xml', '.yaml', '.yml',\n            '.py', '.js', '.ts', '.java', '.cpp', '.c', '.h',\n            '.html', '.css', '.scss', '.sql', '.sh', '.bash',\n            '.log', '.csv', '.tsv', '.rst', '.ini', '.cfg',\n            '.conf', '.config', '.properties', '.env'\n        }\n\n        if path.suffix.lower() in text_extensions:\n            return True\n\n        # Try to read first few bytes\n        try:\n            with open(path, 'rb') as f:\n                chunk = f.read(512)\n                # Check if bytes are mostly printable ASCII\n                if chunk:\n                    text_characters = sum(1 for b in chunk if 32 <= b < 127 or b in [9, 10, 13])\n                    return (text_characters / len(chunk)) > 0.85\n        except:\n            pass\n\n        return False\n\n    def _count_lines_safe(self, path: Path, max_size_mb: int = 50) -> Dict[str, Any]:\n        \"\"\"\n        Safely count lines in a text file with size limits\n\n        Args:\n            path: File path\n            max_size_mb: Maximum file size to fully count (default: 50MB)\n\n        Returns:\n            Dictionary with line count info\n        \"\"\"\n        stat = path.stat()\n        size_mb = stat.st_size / (1024 * 1024)\n\n        # For very large files, estimate instead of counting\n        if size_mb > max_size_mb:\n            # Sample first 10000 lines to estimate\n            try:\n                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n                    sample_lines = []\n                    sample_bytes = 0\n                    for i in range(10000):\n                        line = f.readline()\n                        if not line:\n                            break\n                        sample_lines.append(line)\n                        sample_bytes += len(line.encode('utf-8'))\n\n                    if sample_bytes > 0:\n                        avg_bytes_per_line = sample_bytes / len(sample_lines)\n                        estimated_lines = int(stat.st_size / avg_bytes_per_line)\n\n                        return {\n                            \"total_lines\": None,\n                            \"estimated_lines\": estimated_lines,\n                            \"line_count_method\": \"estimated\",\n                            \"sample_size\": len(sample_lines),\n                            \"avg_line_length_bytes\": int(avg_bytes_per_line)\n                        }\n            except:\n                return {\n                    \"total_lines\": None,\n                    \"estimated_lines\": None,\n                    \"line_count_method\": \"failed\",\n                    \"error\": \"Could not estimate line count\"\n                }\n\n        # For smaller files, count exactly\n        try:\n            encodings = ['utf-8', 'latin-1', 'cp1252']\n            for encoding in encodings:\n                try:\n                    with open(path, 'r', encoding=encoding) as f:\n                        line_count = sum(1 for _ in f)\n\n                    # Get average line length\n                    avg_line_bytes = stat.st_size / line_count if line_count > 0 else 0\n\n                    return {\n                        \"total_lines\": line_count,\n                        \"estimated_lines\": None,\n                        \"line_count_method\": \"exact\",\n                        \"encoding_used\": encoding,\n                        \"avg_line_length_bytes\": int(avg_line_bytes),\n                        \"estimated_tokens\": int(line_count * 15)  # Rough estimate\n                    }\n                except UnicodeDecodeError:\n                    continue\n\n            return {\n                \"total_lines\": None,\n                \"line_count_method\": \"failed\",\n                \"error\": \"Could not decode file\"\n            }\n\n        except Exception as e:\n            return {\n                \"total_lines\": None,\n                \"line_count_method\": \"error\",\n                \"error\": str(e)\n            }\n\n    def _get_directory_info(self, path: Path) -> Dict[str, Any]:\n        \"\"\"\n        Get information about a directory\n\n        Args:\n            path: Directory path\n\n        Returns:\n            Directory information dictionary\n        \"\"\"\n        try:\n            # Count items\n            items = list(path.iterdir())\n            files = [i for i in items if i.is_file()]\n            dirs = [i for i in items if i.is_dir()]\n\n            # Calculate total size\n            total_size = sum(f.stat().st_size for f in files)\n\n            return {\n                \"total_items\": len(items),\n                \"file_count\": len(files),\n                \"directory_count\": len(dirs),\n                \"total_size_bytes\": total_size,\n                \"total_size_mb\": round(total_size / (1024 * 1024), 2),\n                \"file_list\": [f.name for f in files[:20]],  # First 20 files\n                \"subdirectories\": [d.name for d in dirs[:20]]  # First 20 dirs\n            }\n\n        except Exception as e:\n            return {\n                \"error\": f\"Could not read directory: {str(e)}\"\n            }\n\n    def _get_processing_strategy(self, metadata: Dict[str, Any]) -> str:\n        \"\"\"\n        Recommend processing strategy based on file size\n\n        Args:\n            metadata: File metadata\n\n        Returns:\n            Strategy recommendation string\n        \"\"\"\n        if not metadata.get(\"is_text_file\"):\n            return \"NON_TEXT_FILE: This file cannot be processed as text\"\n\n        # Get line count (exact or estimated)\n        lines = metadata.get(\"total_lines\") or metadata.get(\"estimated_lines\", 0)\n        size_mb = metadata.get(\"size_mb\", 0)\n\n        if lines == 0 and size_mb == 0:\n            return \"EMPTY_FILE: File is empty or could not be read\"\n\n        # Define strategies based on size\n        if lines < 100:\n            return \"TINY_FILE: Read entire file at once. No chunking needed.\"\n\n        elif lines < 500:\n            return \"SMALL_FILE: Can read entire file. Chunking optional.\"\n\n        elif lines < 2000:\n            return \"MEDIUM_FILE: Recommended - Use Grep search first for targeted info, OR read in 2-4 chunks of 500 lines each.\"\n\n        elif lines < 5000:\n            return \"LARGE_FILE: Required - Use Grep search to find relevant sections, then read only matching chunks. Avoid full file read.\"\n\n        elif lines < 20000:\n            return \"VERY_LARGE_FILE: Required - ONLY use Grep search. Do NOT attempt full read. Read specific chunks only after Grep identifies locations.\"\n\n        else:\n            return \"MASSIVE_FILE: Critical - GREP SEARCH ONLY. Never attempt to read full file. Process in targeted chunks based on Grep results only.\"\n\n    def get_recommendation(self) -> Data:\n        \"\"\"\n        Get a detailed processing recommendation\n\n        Returns:\n            Data object with recommendation details\n        \"\"\"\n        metadata_result = self.get_metadata()\n        metadata = metadata_result.data\n\n        if not metadata.get(\"exists\"):\n            return Data(data={\n                \"recommendation\": \"ERROR: File does not exist\",\n                \"strategy\": \"none\",\n                \"metadata\": metadata\n            })\n\n        if not metadata.get(\"is_file\"):\n            return Data(data={\n                \"recommendation\": \"This is a directory, not a file\",\n                \"strategy\": \"directory\",\n                \"metadata\": metadata\n            })\n\n        strategy = metadata.get(\"processing_recommendation\", \"UNKNOWN\")\n        lines = metadata.get(\"total_lines\") or metadata.get(\"estimated_lines\", 0)\n\n        # Build detailed recommendation\n        recommendation = {\n            \"strategy\": strategy,\n            \"file_size_mb\": metadata.get(\"size_mb\"),\n            \"line_count\": lines,\n            \"recommended_tools\": self._recommend_tools(lines),\n            \"warnings\": self._get_warnings(metadata),\n            \"suggested_workflow\": self._suggest_workflow(lines),\n            \"metadata\": metadata\n        }\n\n        return Data(data=recommendation)\n\n    def _recommend_tools(self, lines: int) -> List[str]:\n        \"\"\"Recommend which tools to use based on file size\"\"\"\n        if lines < 500:\n            return [\"ChunkedFileReader (single chunk)\", \"Direct read\"]\n        elif lines < 2000:\n            return [\"GrepSearchTool (recommended)\", \"ChunkedFileReader (2-4 chunks)\"]\n        elif lines < 5000:\n            return [\"GrepSearchTool (required)\", \"ChunkedFileReader (targeted chunks only)\"]\n        else:\n            return [\"GrepSearchTool (required - search only)\", \"ChunkedFileReader (small targeted chunks only)\"]\n\n    def _get_warnings(self, metadata: Dict[str, Any]) -> List[str]:\n        \"\"\"Generate warnings based on file characteristics\"\"\"\n        warnings = []\n\n        lines = metadata.get(\"total_lines\") or metadata.get(\"estimated_lines\", 0)\n        size_mb = metadata.get(\"size_mb\", 0)\n\n        if lines > 5000:\n            warnings.append(\"⚠️ Large file: Do not attempt to read entire file at once\")\n\n        if lines > 20000:\n            warnings.append(\"🚨 Very large file: Use Grep search exclusively\")\n\n        if size_mb > 10:\n            warnings.append(\"⚠️ File size >10MB: May be slow to process\")\n\n        if metadata.get(\"line_count_method\") == \"estimated\":\n            warnings.append(\"ℹ️ Line count is estimated, not exact\")\n\n        if not metadata.get(\"is_text_file\"):\n            warnings.append(\"⚠️ Non-text file: Cannot process as text\")\n\n        return warnings\n\n    def _suggest_workflow(self, lines: int) -> Dict[str, Any]:\n        \"\"\"Suggest a specific workflow based on file size\"\"\"\n        if lines < 500:\n            return {\n                \"step_1\": \"Read entire file with ChunkedFileReader (chunk_number=0)\",\n                \"step_2\": \"Process content directly\",\n                \"alternative\": \"None needed for small files\"\n            }\n        elif lines < 2000:\n            return {\n                \"step_1\": \"Use GrepSearchTool to search for specific keywords\",\n                \"step_2\": \"If Grep finds matches, read those specific chunks\",\n                \"step_3\": \"If no matches, read file in 2-4 chunks of 500 lines each\",\n                \"alternative\": \"For general overview, read first and last chunks only\"\n            }\n        else:\n            return {\n                \"step_1\": \"Use GrepSearchTool with output_mode='files_only' to check if file contains relevant info\",\n                \"step_2\": \"Use GrepSearchTool with output_mode='matches_with_context' to see specific matches\",\n                \"step_3\": \"Use ChunkedFileReader to read ONLY the specific chunks containing matches\",\n                \"step_4\": \"Never attempt to read entire file\",\n                \"alternative\": \"If no specific search terms, read first 500 lines for overview, then specific sections as needed\"\n            }\n\n\n# Make component available to Langflow\nif __name__ == \"__main__\":\n    # Test the component\n    tool = FileMetadataTool()\n    tool.file_path = \"test_file.txt\"\n    tool.include_recommendations = True\n    tool.count_lines = True\n\n    metadata = tool.get_metadata()\n    print(\"Metadata:\", metadata.data)\n\n    recommendation = tool.get_recommendation()\n    print(\"\\nRecommendation:\", recommendation.data)\n"
              },
              "count_lines": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Count Lines",
                "dynamic": false,
                "info": "Count total lines in text files (may be slow for large files)",
                "list": false,
                "list_add_label": "Add More",
                "name": "count_lines",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "file_path": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "File Path",
                "dynamic": false,
                "info": "Path to file or directory",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "file_path",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "include_recommendations": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Include Processing Recommendations",
                "dynamic": false,
                "info": "Include suggested processing strategies based on file size",
                "list": false,
                "list_add_label": "Add More",
                "name": "include_recommendations",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "tools_metadata": {
                "_input_type": "ToolsInput",
                "advanced": false,
                "display_name": "Actions",
                "dynamic": false,
                "info": "Modify tool names and descriptions to help agents understand when to use each tool.",
                "is_list": true,
                "list_add_label": "Add More",
                "name": "tools_metadata",
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "tools",
                "value": [
                  {
                    "args": {
                      "file_path": {
                        "description": "Path to file or directory",
                        "title": "File Path",
                        "type": "string"
                      }
                    },
                    "description": "Get file size, type, and metadata to plan processing strategy",
                    "display_description": "Get file size, type, and metadata to plan processing strategy",
                    "display_name": "get_metadata",
                    "name": "get_metadata",
                    "readonly": false,
                    "status": true,
                    "tags": [
                      "get_metadata"
                    ]
                  },
                  {
                    "args": {
                      "file_path": {
                        "description": "Path to file or directory",
                        "title": "File Path",
                        "type": "string"
                      }
                    },
                    "description": "Get file size, type, and metadata to plan processing strategy",
                    "display_description": "Get file size, type, and metadata to plan processing strategy",
                    "display_name": "get_recommendation",
                    "name": "get_recommendation",
                    "readonly": false,
                    "status": true,
                    "tags": [
                      "get_recommendation"
                    ]
                  }
                ]
              }
            },
            "tool_mode": true
          },
          "selected_output": "metadata",
          "showNode": true,
          "type": "FileMetadataTool"
        },
        "dragging": false,
        "id": "CustomComponent-frHjb",
        "measured": {
          "height": 217,
          "width": 320
        },
        "position": {
          "x": 2224.590923358693,
          "y": -507.1053793449611
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "CustomComponent-km8ac",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Search file contents with regex patterns - efficient for large files",
            "display_name": "Grep Search Tool",
            "documentation": "",
            "edited": true,
            "field_order": [
              "file_path",
              "pattern",
              "output_mode",
              "case_sensitive",
              "context_lines",
              "max_matches",
              "recursive",
              "file_pattern"
            ],
            "frozen": false,
            "icon": "search",
            "last_updated": "2025-11-10T20:26:10.241Z",
            "legacy": false,
            "lf_version": "1.6.0",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Toolset",
                "group_outputs": false,
                "hidden": null,
                "method": "to_toolkit",
                "name": "component_as_tool",
                "options": null,
                "required_inputs": null,
                "selected": "Tool",
                "tool_mode": true,
                "types": [
                  "Tool"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "case_sensitive": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Case Sensitive",
                "dynamic": false,
                "info": "Whether the search should be case-sensitive",
                "list": false,
                "list_add_label": "Add More",
                "name": "case_sensitive",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "\"\"\"\nGrep Search Tool for Langflow\nEnables efficient file content searching without reading entire files\n\"\"\"\n\nfrom langflow.custom import Component\nfrom langflow.io import StrInput, BoolInput, IntInput, DropdownInput, Output\nfrom langflow.schema import Data\nimport re\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\n\nclass GrepSearchTool(Component):\n    \"\"\"\n    Search file contents using regex patterns without loading entire files into memory.\n    This tool is essential for agents working with large documents.\n    \"\"\"\n\n    display_name = \"Grep Search Tool\"\n    description = \"Search file contents with regex patterns - efficient for large files\"\n    icon = \"search\"\n    name = \"GrepSearchTool\"\n\n    inputs = [\n        StrInput(\n            name=\"file_path\",\n            display_name=\"File Path\",\n            info=\"Path to the file to search (or directory for recursive search)\",\n            required=True,\n            tool_mode=True\n        ),\n        StrInput(\n            name=\"pattern\",\n            display_name=\"Search Pattern\",\n            info=\"Regex pattern to search for (e.g., 'error|warning', 'function\\\\s+\\\\w+')\",\n            required=True,\n            tool_mode=True,\n            value=\"\"\n        ),\n        DropdownInput(\n            name=\"output_mode\",\n            display_name=\"Output Mode\",\n            info=\"What to return from the search\",\n            options=[\"matches_with_context\", \"files_only\", \"count_only\", \"line_numbers\"],\n            value=\"matches_with_context\",\n            advanced=False\n        ),\n        BoolInput(\n            name=\"case_sensitive\",\n            display_name=\"Case Sensitive\",\n            value=False,\n            info=\"Whether the search should be case-sensitive\",\n            advanced=True\n        ),\n        IntInput(\n            name=\"context_lines\",\n            display_name=\"Context Lines\",\n            value=3,\n            info=\"Number of lines before/after match to include\",\n            advanced=True\n        ),\n        IntInput(\n            name=\"max_matches\",\n            display_name=\"Max Matches\",\n            value=100,\n            info=\"Maximum number of matches to return\",\n            advanced=True\n        ),\n        BoolInput(\n            name=\"recursive\",\n            display_name=\"Recursive Search\",\n            value=False,\n            info=\"Search in subdirectories if file_path is a directory\",\n            advanced=True\n        ),\n        StrInput(\n            name=\"file_pattern\",\n            display_name=\"File Pattern Filter\",\n            value=\"*\",\n            info=\"Glob pattern for files to search (e.g., '*.py', '*.txt')\",\n            advanced=True\n        )\n    ]\n\n    outputs = [\n        Output(\n            name=\"search_results\",\n            display_name=\"Search Results\",\n            method=\"search\"\n        ),\n        Output(\n            name=\"summary\",\n            display_name=\"Search Summary\",\n            method=\"get_summary\"\n        )\n    ]\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.matches: List[Dict[str, Any]] = []\n        self.files_searched: int = 0\n        self.total_matches: int = 0\n\n    def search(self) -> Data:\n        \"\"\"\n        Execute the search operation\n\n        Returns:\n            Data object containing search results\n        \"\"\"\n        try:\n            # Reset state\n            self.matches = []\n            self.files_searched = 0\n            self.total_matches = 0\n\n            # Compile regex pattern\n            flags = 0 if self.case_sensitive else re.IGNORECASE\n            try:\n                compiled_pattern = re.compile(self.pattern, flags)\n            except re.error as e:\n                return Data(data={\n                    \"error\": f\"Invalid regex pattern: {str(e)}\",\n                    \"pattern\": self.pattern\n                })\n\n            # Determine if searching file or directory\n            path = Path(self.file_path)\n\n            if not path.exists():\n                return Data(data={\n                    \"error\": f\"Path does not exist: {self.file_path}\",\n                    \"matches\": []\n                })\n\n            if path.is_file():\n                # Search single file\n                self._search_file(path, compiled_pattern)\n            elif path.is_dir():\n                # Search directory\n                if self.recursive:\n                    # Recursive search\n                    for file_path in path.rglob(self.file_pattern):\n                        if file_path.is_file():\n                            self._search_file(file_path, compiled_pattern)\n                            if self.total_matches >= self.max_matches:\n                                break\n                else:\n                    # Non-recursive search\n                    for file_path in path.glob(self.file_pattern):\n                        if file_path.is_file():\n                            self._search_file(file_path, compiled_pattern)\n                            if self.total_matches >= self.max_matches:\n                                break\n\n            # Format results based on output mode\n            result_data = self._format_results()\n\n            # Update status\n            self.status = f\"Found {self.total_matches} matches in {self.files_searched} file(s)\"\n\n            return Data(data=result_data)\n\n        except Exception as e:\n            self.log(f\"Search error: {str(e)}\")\n            return Data(data={\n                \"error\": str(e),\n                \"matches\": [],\n                \"total_matches\": 0\n            })\n\n    def _search_file(self, file_path: Path, pattern: re.Pattern):\n        \"\"\"\n        Search a single file for the pattern\n\n        Args:\n            file_path: Path to the file\n            pattern: Compiled regex pattern\n        \"\"\"\n        try:\n            # Try to read file with different encodings\n            encodings = ['utf-8', 'latin-1', 'cp1252']\n            lines = None\n\n            for encoding in encodings:\n                try:\n                    with open(file_path, 'r', encoding=encoding) as f:\n                        lines = f.readlines()\n                    break\n                except UnicodeDecodeError:\n                    continue\n\n            if lines is None:\n                self.log(f\"Could not decode file: {file_path}\")\n                return\n\n            self.files_searched += 1\n            file_matches = []\n\n            # Search each line\n            for line_num, line in enumerate(lines, start=1):\n                if pattern.search(line):\n                    # Get context lines\n                    context_start = max(0, line_num - 1 - self.context_lines)\n                    context_end = min(len(lines), line_num + self.context_lines)\n\n                    context_lines = []\n                    for i in range(context_start, context_end):\n                        prefix = \">>> \" if i == line_num - 1 else \"    \"\n                        context_lines.append(f\"{prefix}{i+1:5d} | {lines[i].rstrip()}\")\n\n                    match_info = {\n                        \"file\": str(file_path),\n                        \"line_number\": line_num,\n                        \"line_text\": line.rstrip(),\n                        \"context\": \"\\n\".join(context_lines)\n                    }\n\n                    file_matches.append(match_info)\n                    self.total_matches += 1\n\n                    # Stop if max matches reached\n                    if self.total_matches >= self.max_matches:\n                        break\n\n            if file_matches:\n                self.matches.extend(file_matches)\n\n        except Exception as e:\n            self.log(f\"Error searching file {file_path}: {str(e)}\")\n\n    def _format_results(self) -> Dict[str, Any]:\n        \"\"\"\n        Format results based on output mode\n\n        Returns:\n            Formatted result dictionary\n        \"\"\"\n        if self.output_mode == \"files_only\":\n            # Return only unique file paths\n            unique_files = list(set(m[\"file\"] for m in self.matches))\n            return {\n                \"files\": unique_files,\n                \"total_files\": len(unique_files),\n                \"pattern\": self.pattern,\n                \"mode\": \"files_only\"\n            }\n\n        elif self.output_mode == \"count_only\":\n            # Return counts per file\n            counts = {}\n            for match in self.matches:\n                file = match[\"file\"]\n                counts[file] = counts.get(file, 0) + 1\n\n            return {\n                \"counts\": counts,\n                \"total_matches\": self.total_matches,\n                \"total_files\": len(counts),\n                \"pattern\": self.pattern,\n                \"mode\": \"count_only\"\n            }\n\n        elif self.output_mode == \"line_numbers\":\n            # Return file and line numbers only\n            results = [\n                {\n                    \"file\": m[\"file\"],\n                    \"line\": m[\"line_number\"],\n                    \"text\": m[\"line_text\"]\n                }\n                for m in self.matches\n            ]\n            return {\n                \"matches\": results,\n                \"total_matches\": self.total_matches,\n                \"pattern\": self.pattern,\n                \"mode\": \"line_numbers\"\n            }\n\n        else:  # matches_with_context (default)\n            return {\n                \"matches\": self.matches,\n                \"total_matches\": self.total_matches,\n                \"files_searched\": self.files_searched,\n                \"pattern\": self.pattern,\n                \"mode\": \"matches_with_context\",\n                \"case_sensitive\": self.case_sensitive,\n                \"truncated\": self.total_matches >= self.max_matches\n            }\n\n    def get_summary(self) -> Data:\n        \"\"\"\n        Get a text summary of the search results\n\n        Returns:\n            Data object with formatted summary text\n        \"\"\"\n        if not self.matches:\n            summary = f\"No matches found for pattern: '{self.pattern}'\"\n        else:\n            summary_lines = [\n                f\"Search Results for pattern: '{self.pattern}'\",\n                f\"=\" * 60,\n                f\"Total matches: {self.total_matches}\",\n                f\"Files searched: {self.files_searched}\",\n                f\"Files with matches: {len(set(m['file'] for m in self.matches))}\",\n                \"\",\n                \"Matches by file:\",\n                \"\"\n            ]\n\n            # Group matches by file\n            by_file = {}\n            for match in self.matches:\n                file = match[\"file\"]\n                if file not in by_file:\n                    by_file[file] = []\n                by_file[file].append(match)\n\n            # Format each file's matches\n            for file, file_matches in by_file.items():\n                summary_lines.append(f\"\\n{file} ({len(file_matches)} matches):\")\n                summary_lines.append(\"-\" * 60)\n\n                for match in file_matches[:5]:  # Show first 5 matches per file\n                    summary_lines.append(f\"\\nLine {match['line_number']}:\")\n                    summary_lines.append(match['context'])\n\n                if len(file_matches) > 5:\n                    summary_lines.append(f\"\\n... and {len(file_matches) - 5} more matches\")\n\n            summary = \"\\n\".join(summary_lines)\n\n        return Data(data={\"summary\": summary, \"text\": summary})\n\n\n# Make the component available to Langflow\nif __name__ == \"__main__\":\n    # Test the component\n    tool = GrepSearchTool()\n    tool.file_path = \"test_file.txt\"\n    tool.pattern = \"error\"\n    tool.output_mode = \"matches_with_context\"\n    tool.case_sensitive = False\n    tool.context_lines = 2\n    tool.max_matches = 50\n\n    result = tool.search()\n    print(result.data)\n"
              },
              "context_lines": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Context Lines",
                "dynamic": false,
                "info": "Number of lines before/after match to include",
                "list": false,
                "list_add_label": "Add More",
                "name": "context_lines",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 3
              },
              "file_path": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "File Path",
                "dynamic": false,
                "info": "Path to the file to search (or directory for recursive search)",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "file_path",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "file_pattern": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "File Pattern Filter",
                "dynamic": false,
                "info": "Glob pattern for files to search (e.g., '*.py', '*.txt')",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "file_pattern",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "*"
              },
              "max_matches": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Matches",
                "dynamic": false,
                "info": "Maximum number of matches to return",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_matches",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 100
              },
              "output_mode": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Output Mode",
                "dynamic": false,
                "external_options": {},
                "info": "What to return from the search",
                "name": "output_mode",
                "options": [
                  "matches_with_context",
                  "files_only",
                  "count_only",
                  "line_numbers"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "matches_with_context"
              },
              "pattern": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Search Pattern",
                "dynamic": false,
                "info": "Regex pattern to search for (e.g., 'error|warning', 'function\\s+\\w+')",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "pattern",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "recursive": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Recursive Search",
                "dynamic": false,
                "info": "Search in subdirectories if file_path is a directory",
                "list": false,
                "list_add_label": "Add More",
                "name": "recursive",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "tools_metadata": {
                "_input_type": "ToolsInput",
                "advanced": false,
                "display_name": "Actions",
                "dynamic": false,
                "info": "Modify tool names and descriptions to help agents understand when to use each tool.",
                "is_list": true,
                "list_add_label": "Add More",
                "name": "tools_metadata",
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "tools",
                "value": [
                  {
                    "args": {
                      "file_path": {
                        "description": "Path to the file to search (or directory for recursive search)",
                        "title": "File Path",
                        "type": "string"
                      },
                      "pattern": {
                        "description": "Regex pattern to search for (e.g., 'error|warning', 'function\\s+\\w+')",
                        "title": "Pattern",
                        "type": "string"
                      }
                    },
                    "description": "Search file contents with regex patterns - efficient for large files",
                    "display_description": "Search file contents with regex patterns - efficient for large files",
                    "display_name": "search",
                    "name": "search",
                    "readonly": false,
                    "status": true,
                    "tags": [
                      "search"
                    ]
                  },
                  {
                    "args": {
                      "file_path": {
                        "description": "Path to the file to search (or directory for recursive search)",
                        "title": "File Path",
                        "type": "string"
                      },
                      "pattern": {
                        "description": "Regex pattern to search for (e.g., 'error|warning', 'function\\s+\\w+')",
                        "title": "Pattern",
                        "type": "string"
                      }
                    },
                    "description": "Search file contents with regex patterns - efficient for large files",
                    "display_description": "Search file contents with regex patterns - efficient for large files",
                    "display_name": "get_summary",
                    "name": "get_summary",
                    "readonly": false,
                    "status": true,
                    "tags": [
                      "get_summary"
                    ]
                  }
                ]
              }
            },
            "tool_mode": true
          },
          "selected_output": "search_results",
          "showNode": true,
          "type": "GrepSearchTool"
        },
        "dragging": false,
        "id": "CustomComponent-km8ac",
        "measured": {
          "height": 299,
          "width": 320
        },
        "position": {
          "x": 2226.057305670236,
          "y": -239.80100336667357
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "CustomComponent-dSqdc",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Read large files in manageable chunks with overlap support",
            "display_name": "Chunked File Reader",
            "documentation": "",
            "edited": true,
            "field_order": [
              "file_path",
              "chunk_number",
              "chunk_size",
              "overlap_lines",
              "include_line_numbers",
              "encoding"
            ],
            "frozen": false,
            "icon": "file-text",
            "last_updated": "2025-11-10T20:26:10.241Z",
            "legacy": false,
            "lf_version": "1.6.0",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Toolset",
                "group_outputs": false,
                "hidden": null,
                "method": "to_toolkit",
                "name": "component_as_tool",
                "options": null,
                "required_inputs": null,
                "selected": "Tool",
                "tool_mode": true,
                "types": [
                  "Tool"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "chunk_number": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Chunk Number",
                "dynamic": false,
                "info": "Which chunk to read (0-indexed)",
                "list": false,
                "list_add_label": "Add More",
                "name": "chunk_number",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_metadata": true,
                "type": "int",
                "value": 0
              },
              "chunk_size": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Chunk Size (lines)",
                "dynamic": false,
                "info": "Number of lines per chunk (default: 500)",
                "list": false,
                "list_add_label": "Add More",
                "name": "chunk_size",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 500
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "\"\"\"\nChunked File Reader for Langflow\nEnables reading large files in manageable chunks to avoid context window overflow\n\"\"\"\n\nfrom langflow.custom import Component\nfrom langflow.io import StrInput, IntInput, BoolInput, Output\nfrom langflow.schema import Data\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Dict, Any\n\n\nclass ChunkedFileReader(Component):\n    \"\"\"\n    Read large files in chunks to avoid overwhelming the model's context window.\n    Essential for processing documents larger than the model's token limit.\n    \"\"\"\n\n    display_name = \"Chunked File Reader\"\n    description = \"Read large files in manageable chunks with overlap support\"\n    icon = \"file-text\"\n    name = \"ChunkedFileReader\"\n\n    inputs = [\n        StrInput(\n            name=\"file_path\",\n            display_name=\"File Path\",\n            info=\"Path to the file to read\",\n            required=True,\n            tool_mode=True\n        ),\n        IntInput(\n            name=\"chunk_number\",\n            display_name=\"Chunk Number\",\n            value=0,\n            info=\"Which chunk to read (0-indexed)\",\n            tool_mode=True\n        ),\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size (lines)\",\n            value=500,\n            info=\"Number of lines per chunk (default: 500)\",\n            advanced=False\n        ),\n        IntInput(\n            name=\"overlap_lines\",\n            display_name=\"Overlap Lines\",\n            value=50,\n            info=\"Number of lines to overlap between chunks for context preservation\",\n            advanced=True\n        ),\n        BoolInput(\n            name=\"include_line_numbers\",\n            display_name=\"Include Line Numbers\",\n            value=True,\n            info=\"Prefix each line with its line number\",\n            advanced=True\n        ),\n        StrInput(\n            name=\"encoding\",\n            display_name=\"File Encoding\",\n            value=\"utf-8\",\n            info=\"File encoding (utf-8, latin-1, cp1252, etc.)\",\n            advanced=True\n        )\n    ]\n\n    outputs = [\n        Output(\n            name=\"chunk_data\",\n            display_name=\"Chunk Data\",\n            method=\"read_chunk\"\n        ),\n        Output(\n            name=\"file_info\",\n            display_name=\"File Information\",\n            method=\"get_file_info\"\n        )\n    ]\n\n    def read_chunk(self) -> Data:\n        \"\"\"\n        Read a specific chunk from the file\n\n        Returns:\n            Data object containing chunk content and metadata\n        \"\"\"\n        try:\n            file_path = Path(self.file_path)\n\n            if not file_path.exists():\n                return Data(data={\n                    \"error\": f\"File not found: {self.file_path}\",\n                    \"exists\": False\n                })\n\n            if not file_path.is_file():\n                return Data(data={\n                    \"error\": f\"Path is not a file: {self.file_path}\",\n                    \"exists\": True,\n                    \"is_directory\": True\n                })\n\n            # Calculate line ranges\n            # Account for overlap from previous chunk\n            actual_start_line = max(0, (self.chunk_number * self.chunk_size) - self.overlap_lines)\n            actual_end_line = actual_start_line + self.chunk_size + self.overlap_lines\n\n            # For chunk 0, don't include overlap at start\n            if self.chunk_number == 0:\n                actual_start_line = 0\n                actual_end_line = self.chunk_size\n\n            # Try to read with specified encoding, fallback to others\n            encodings_to_try = [self.encoding, 'utf-8', 'latin-1', 'cp1252']\n            lines = None\n            encoding_used = None\n\n            for enc in encodings_to_try:\n                try:\n                    with open(file_path, 'r', encoding=enc) as f:\n                        all_lines = f.readlines()\n                    lines = all_lines\n                    encoding_used = enc\n                    break\n                except (UnicodeDecodeError, LookupError):\n                    continue\n\n            if lines is None:\n                return Data(data={\n                    \"error\": f\"Could not decode file with any supported encoding\",\n                    \"tried_encodings\": encodings_to_try\n                })\n\n            total_lines = len(lines)\n\n            # Extract the requested chunk\n            chunk_lines = lines[actual_start_line:actual_end_line]\n\n            # Format with line numbers if requested\n            if self.include_line_numbers:\n                formatted_lines = []\n                for i, line in enumerate(chunk_lines):\n                    line_num = actual_start_line + i + 1\n                    formatted_lines.append(f\"{line_num:6d} | {line.rstrip()}\")\n                content = \"\\n\".join(formatted_lines)\n            else:\n                content = \"\".join(chunk_lines)\n\n            # Determine if there are more chunks\n            has_more = actual_end_line < total_lines\n            next_chunk = self.chunk_number + 1 if has_more else None\n\n            # Calculate progress percentage\n            progress = min(100, int((actual_end_line / total_lines) * 100)) if total_lines > 0 else 100\n\n            # Identify overlap region\n            overlap_start = self.overlap_lines if self.chunk_number > 0 else 0\n            overlap_end = len(chunk_lines)\n\n            result_data = {\n                \"content\": content,\n                \"chunk_number\": self.chunk_number,\n                \"start_line\": actual_start_line + 1,  # 1-indexed for user\n                \"end_line\": actual_start_line + len(chunk_lines),\n                \"lines_in_chunk\": len(chunk_lines),\n                \"total_lines_in_file\": total_lines,\n                \"has_more_chunks\": has_more,\n                \"next_chunk_number\": next_chunk,\n                \"progress_percent\": progress,\n                \"encoding_used\": encoding_used,\n                \"file_path\": str(file_path),\n                \"file_name\": file_path.name,\n                \"overlap_lines\": self.overlap_lines if self.chunk_number > 0 else 0,\n                \"is_last_chunk\": not has_more,\n                \"is_first_chunk\": self.chunk_number == 0\n            }\n\n            # Update status\n            status_msg = f\"Read chunk {self.chunk_number}: lines {result_data['start_line']}-{result_data['end_line']} \"\n            status_msg += f\"({progress}% complete)\"\n            self.status = status_msg\n\n            return Data(data=result_data)\n\n        except Exception as e:\n            self.log(f\"Error reading chunk: {str(e)}\")\n            return Data(data={\n                \"error\": str(e),\n                \"chunk_number\": self.chunk_number,\n                \"file_path\": self.file_path\n            })\n\n    def get_file_info(self) -> Data:\n        \"\"\"\n        Get metadata about the file\n\n        Returns:\n            Data object with file information\n        \"\"\"\n        try:\n            file_path = Path(self.file_path)\n\n            if not file_path.exists():\n                return Data(data={\n                    \"exists\": False,\n                    \"error\": \"File not found\"\n                })\n\n            # Get file stats\n            stat = file_path.stat()\n\n            # Count lines\n            encodings_to_try = [self.encoding, 'utf-8', 'latin-1', 'cp1252']\n            total_lines = 0\n            encoding_used = None\n\n            for enc in encodings_to_try:\n                try:\n                    with open(file_path, 'r', encoding=enc) as f:\n                        total_lines = sum(1 for _ in f)\n                    encoding_used = enc\n                    break\n                except (UnicodeDecodeError, LookupError):\n                    continue\n\n            if total_lines == 0 and encoding_used is None:\n                return Data(data={\n                    \"exists\": True,\n                    \"error\": \"Could not decode file\",\n                    \"size_bytes\": stat.st_size\n                })\n\n            # Calculate chunking information\n            total_chunks = (total_lines + self.chunk_size - 1) // self.chunk_size\n            estimated_tokens = total_lines * 15  # Rough estimate: ~15 tokens per line\n\n            info = {\n                \"exists\": True,\n                \"file_path\": str(file_path),\n                \"file_name\": file_path.name,\n                \"size_bytes\": stat.st_size,\n                \"size_kb\": stat.st_size / 1024,\n                \"size_mb\": stat.st_size / (1024 * 1024),\n                \"total_lines\": total_lines,\n                \"encoding\": encoding_used,\n                \"modified_timestamp\": stat.st_mtime,\n                \"chunking_info\": {\n                    \"chunk_size\": self.chunk_size,\n                    \"overlap_lines\": self.overlap_lines,\n                    \"total_chunks\": total_chunks,\n                    \"estimated_total_tokens\": estimated_tokens,\n                    \"recommended_strategy\": self._recommend_strategy(total_lines, estimated_tokens)\n                }\n            }\n\n            self.status = f\"File: {total_lines} lines, {total_chunks} chunks needed\"\n\n            return Data(data=info)\n\n        except Exception as e:\n            self.log(f\"Error getting file info: {str(e)}\")\n            return Data(data={\n                \"error\": str(e),\n                \"file_path\": self.file_path\n            })\n\n    def _recommend_strategy(self, total_lines: int, estimated_tokens: int) -> str:\n        \"\"\"\n        Recommend a reading strategy based on file size\n\n        Args:\n            total_lines: Total lines in file\n            estimated_tokens: Estimated token count\n\n        Returns:\n            Strategy recommendation string\n        \"\"\"\n        if total_lines < 100:\n            return \"SMALL_FILE: Read entire file at once\"\n        elif total_lines < 1000:\n            return \"MEDIUM_FILE: Read in 1-2 chunks or search with Grep first\"\n        elif total_lines < 5000:\n            return \"LARGE_FILE: Use chunked reading (3-10 chunks) or Grep search\"\n        elif total_lines < 20000:\n            return \"VERY_LARGE_FILE: Use Grep search first, then targeted chunk reading\"\n        else:\n            return \"MASSIVE_FILE: Use Grep search exclusively, avoid full read\"\n\n\nclass StreamingFileReader(Component):\n    \"\"\"\n    Alternative reader that streams file content line by line\n    Useful for processing files without loading entire chunks into memory\n    \"\"\"\n\n    display_name = \"Streaming File Reader\"\n    description = \"Stream file content line by line for memory-efficient processing\"\n    icon = \"flow\"\n    name = \"StreamingFileReader\"\n\n    inputs = [\n        StrInput(\n            name=\"file_path\",\n            display_name=\"File Path\",\n            required=True,\n            tool_mode=True\n        ),\n        IntInput(\n            name=\"max_lines\",\n            display_name=\"Max Lines to Read\",\n            value=1000,\n            info=\"Maximum lines to read in this call\",\n            tool_mode=True\n        ),\n        IntInput(\n            name=\"skip_lines\",\n            display_name=\"Skip Lines\",\n            value=0,\n            info=\"Number of lines to skip from start\",\n            tool_mode=True\n        )\n    ]\n\n    outputs = [\n        Output(\n            name=\"lines\",\n            display_name=\"File Lines\",\n            method=\"read_lines\"\n        )\n    ]\n\n    def read_lines(self) -> Data:\n        \"\"\"\n        Read lines from file with skip and limit\n\n        Returns:\n            Data object with lines array\n        \"\"\"\n        try:\n            with open(self.file_path, 'r', encoding='utf-8') as f:\n                # Skip lines\n                for _ in range(self.skip_lines):\n                    if not f.readline():\n                        break\n\n                # Read requested lines\n                lines = []\n                for i in range(self.max_lines):\n                    line = f.readline()\n                    if not line:\n                        break\n                    lines.append({\n                        \"line_number\": self.skip_lines + i + 1,\n                        \"content\": line.rstrip()\n                    })\n\n            return Data(data={\n                \"lines\": lines,\n                \"count\": len(lines),\n                \"start_line\": self.skip_lines + 1,\n                \"end_line\": self.skip_lines + len(lines)\n            })\n\n        except Exception as e:\n            return Data(data={\"error\": str(e), \"lines\": []})\n\n\n# Make components available to Langflow\nif __name__ == \"__main__\":\n    # Test the component\n    reader = ChunkedFileReader()\n    reader.file_path = \"test_file.txt\"\n    reader.chunk_number = 0\n    reader.chunk_size = 500\n    reader.overlap_lines = 50\n    reader.include_line_numbers = True\n\n    # Get file info\n    info = reader.get_file_info()\n    print(\"File Info:\", info.data)\n\n    # Read first chunk\n    chunk = reader.read_chunk()\n    print(\"\\nChunk Data:\", chunk.data.get(\"chunk_number\"), \"lines:\", chunk.data.get(\"lines_in_chunk\"))\n"
              },
              "encoding": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "File Encoding",
                "dynamic": false,
                "info": "File encoding (utf-8, latin-1, cp1252, etc.)",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "encoding",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "utf-8"
              },
              "file_path": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "File Path",
                "dynamic": false,
                "info": "Path to the file to read",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "file_path",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "include_line_numbers": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Include Line Numbers",
                "dynamic": false,
                "info": "Prefix each line with its line number",
                "list": false,
                "list_add_label": "Add More",
                "name": "include_line_numbers",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "overlap_lines": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Overlap Lines",
                "dynamic": false,
                "info": "Number of lines to overlap between chunks for context preservation",
                "list": false,
                "list_add_label": "Add More",
                "name": "overlap_lines",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 50
              },
              "tools_metadata": {
                "_input_type": "ToolsInput",
                "advanced": false,
                "display_name": "Actions",
                "dynamic": false,
                "info": "Modify tool names and descriptions to help agents understand when to use each tool.",
                "is_list": true,
                "list_add_label": "Add More",
                "name": "tools_metadata",
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "tools",
                "value": [
                  {
                    "args": {
                      "chunk_number": {
                        "default": 0,
                        "description": "Which chunk to read (0-indexed)",
                        "title": "Chunk Number",
                        "type": "integer"
                      },
                      "file_path": {
                        "description": "Path to the file to read",
                        "title": "File Path",
                        "type": "string"
                      }
                    },
                    "description": "Read large files in manageable chunks with overlap support",
                    "display_description": "Read large files in manageable chunks with overlap support",
                    "display_name": "read_chunk",
                    "name": "read_chunk",
                    "readonly": false,
                    "status": true,
                    "tags": [
                      "read_chunk"
                    ]
                  },
                  {
                    "args": {
                      "chunk_number": {
                        "default": 0,
                        "description": "Which chunk to read (0-indexed)",
                        "title": "Chunk Number",
                        "type": "integer"
                      },
                      "file_path": {
                        "description": "Path to the file to read",
                        "title": "File Path",
                        "type": "string"
                      }
                    },
                    "description": "Read large files in manageable chunks with overlap support",
                    "display_description": "Read large files in manageable chunks with overlap support",
                    "display_name": "get_file_info",
                    "name": "get_file_info",
                    "readonly": false,
                    "status": true,
                    "tags": [
                      "get_file_info"
                    ]
                  }
                ]
              }
            },
            "tool_mode": true
          },
          "selected_output": "chunk_data",
          "showNode": true,
          "type": "ChunkedFileReader"
        },
        "dragging": false,
        "id": "CustomComponent-dSqdc",
        "measured": {
          "height": 299,
          "width": 320
        },
        "position": {
          "x": 2228.220701968052,
          "y": 107.66381471907914
        },
        "selected": false,
        "type": "genericNode"
      }
    ],
    "viewport": {
      "x": -726.9614948345675,
      "y": 743.2443917246738,
      "zoom": 0.696255979096425
    }
  },
  "description": "A simple but powerful starter agent.",
  "endpoint_name": null,
  "id": "d18010b3-af17-4a9d-9ac4-64c2e2e4d04c",
  "is_component": false,
  "last_tested_version": "1.6.0",
  "name": "Document Researcher Agent",
  "tags": [
    "assistants",
    "agents"
  ]
}